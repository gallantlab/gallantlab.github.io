<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>About on Gallant Lab</title>
    <link>http://localhost:4001/</link>
    <description>Recent content in About on Gallant Lab</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 17 Sep 2025 12:00:00 -0800</lastBuildDate>
    <atom:link href="http://localhost:4001/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>People</title>
      <link>http://localhost:4001/people/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:4001/people/</guid>
      <description>&lt;h2 id=&#34;principal-investigator&#34;&gt;Principal Investigator&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;div class=&#34;people-grid&#34;&gt;&#xA;  &#xA;  &lt;div class=&#34;person-card card fade-in&#34;&gt;&#xA;    &lt;div class=&#34;person-image&#34;&gt;&#xA;      &#xA;      &lt;img src=&#34;http://localhost:4001/img/people/Gallant.Headshot.1.5x1.webp&#34; alt=&#34;Jack Gallant, PhD&#34; class=&#34;person-img-hover&#34;&gt;&#xA;      &#xA;      &lt;img src=&#34;http://localhost:4001/img/people/Regression.Jack.webp&#34; alt=&#34;Jack Gallant, PhD&#34; class=&#34;person-img-main&#34;&gt;&#xA;    &lt;/div&gt;&#xA;    &lt;div class=&#34;person-info&#34;&gt;&#xA;      &lt;h3 class=&#34;person-name&#34;&gt;Jack Gallant, PhD&lt;/h3&gt;&#xA;      &lt;p class=&#34;person-title&#34;&gt;Head of Lab&lt;/p&gt;&#xA;      &lt;div class=&#34;person-description&#34;&gt;Neuroscience Department&lt;br&gt;&#xA;1 Barker Hall&lt;br&gt;&#xA;Berkeley, CA 94720-3990&lt;br&gt;&#xA;Email: &lt;a href=&#34;mailto:gallant@berkeley.edu&#34;&gt;gallant@berkeley.edu&lt;/a&gt;&#xA;&lt;/div&gt;&#xA;    &lt;/div&gt;&#xA;  &lt;/div&gt;&#xA;  &#xA;&lt;/div&gt;&#xA;&#xA;&lt;h2 id=&#34;current-lab-members&#34;&gt;Current Lab Members&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&lt;div class=&#34;people-grid&#34;&gt;&#xA;  &#xA;  &lt;div class=&#34;person-card card fade-in&#34;&gt;&#xA;    &lt;div class=&#34;person-image&#34;&gt;&#xA;      &#xA;      &lt;img src=&#34;http://localhost:4001/img/people/Matteo.ViscontidOC.webp&#34; alt=&#34;Matteo Visconti di Oleggio Castello, PhD&#34; class=&#34;person-img-main&#34;&gt;&#xA;    &lt;/div&gt;&#xA;    &lt;div class=&#34;person-info&#34;&gt;&#xA;      &lt;h3 class=&#34;person-name&#34;&gt;Matteo Visconti di Oleggio Castello, PhD&lt;/h3&gt;&#xA;      &lt;p class=&#34;person-title&#34;&gt;Senior Postdoc&lt;/p&gt;&#xA;      &lt;div class=&#34;person-description&#34;&gt;Areas of interest include individual differences, clinical applications of functional imaging, and fMRI methods development. (Personal web site &lt;a href=&#39;https://matteovisconti.com/&#39;&gt;here&lt;/a&gt;.)&lt;/div&gt;&#xA;    &lt;/div&gt;&#xA;  &lt;/div&gt;&#xA;  &#xA;  &lt;div class=&#34;person-card card fade-in&#34;&gt;&#xA;    &lt;div class=&#34;person-image&#34;&gt;&#xA;      &#xA;      &lt;img src=&#34;http://localhost:4001/img/people/Tianjiao.Zhang.webp&#34; alt=&#34;Tianjiao Zhang, PhD&#34; class=&#34;person-img-main&#34;&gt;&#xA;    &lt;/div&gt;&#xA;    &lt;div class=&#34;person-info&#34;&gt;&#xA;      &lt;h3 class=&#34;person-name&#34;&gt;Tianjiao Zhang, PhD&lt;/h3&gt;&#xA;      &lt;p class=&#34;person-title&#34;&gt;Postdoc&lt;/p&gt;&#xA;      &lt;div class=&#34;person-description&#34;&gt;Areas of interest include studies of the human navigation system, and fMRI studies in VR.&lt;/div&gt;&#xA;    &lt;/div&gt;&#xA;  &lt;/div&gt;&#xA;  &#xA;  &lt;div class=&#34;person-card card fade-in&#34;&gt;&#xA;    &lt;div class=&#34;person-image&#34;&gt;&#xA;      &#xA;      &lt;img src=&#34;http://localhost:4001/img/people/Evi.Hendrikx.webp&#34; alt=&#34;Evi Hendrikx, PhD&#34; class=&#34;person-img-main&#34;&gt;&#xA;    &lt;/div&gt;&#xA;    &lt;div class=&#34;person-info&#34;&gt;&#xA;      &lt;h3 class=&#34;person-name&#34;&gt;Evi Hendrikx, PhD&lt;/h3&gt;&#xA;      &lt;p class=&#34;person-title&#34;&gt;Postdoc&lt;/p&gt;</description>
    </item>
    <item>
      <title>Publications</title>
      <link>http://localhost:4001/publications/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:4001/publications/</guid>
      <description>&lt;h2 id=&#34;selected-publications&#34;&gt;Selected Publications&lt;/h2&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;div class=&#34;publication-card card fade-in&#34;&gt;&#xA;  &lt;div class=&#34;publication-image&#34;&gt;&#xA;    &lt;a href=&#34;https://www.psyarxiv.com/nt2jq&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&#xA;      &lt;img src=&#34;http://localhost:4001/img/papers/ViscontidOC.Deniz.2025.webp&#34; alt=&#34;Visconti di Oleggio Castello et al. 2025 VEM framework&#34;&gt;&#xA;    &lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;  &lt;div class=&#34;publication-info&#34;&gt;&#xA;    &lt;h3 class=&#34;publication-title&#34;&gt;&#xA;      &lt;a href=&#34;https://www.psyarxiv.com/nt2jq&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Encoding models in functional magnetic resonance imaging: the Voxelwise Encoding Model framework (Visconti di Oleggio Castello, Deniz, et al., PsyArXiv preprint)&lt;/a&gt;&#xA;    &lt;/h3&gt;&#xA;    &lt;p class=&#34;publication-date&#34;&gt;2025-09-17&lt;/p&gt;&#xA;    &lt;div class=&#34;publication-description&#34;&gt;This paper provides the first comprehensive guide for creating encoding models with fMRI data, and complements our VEM tutorials. The Voxelwise Encoding Model (VEM) framework extracts features from stimuli or tasks and uses them in encoding models to predict brain activity. When models successfully predict activity in brain regions, we can conclude that information represented in the features is also encoded in those regions. This comprehensive guide makes this powerful methodology accessible to researchers at all levels.&lt;/div&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;div class=&#34;publication-card card fade-in&#34;&gt;&#xA;  &lt;div class=&#34;publication-image&#34;&gt;&#xA;    &lt;a href=&#34;https://www.biorxiv.org/content/biorxiv/early/2024/11/21/2024.06.24.600505.full.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&#xA;      &lt;img src=&#34;http://localhost:4001/img/papers/Chen.etal.2024.2.webp&#34; alt=&#34;Chen bilingual 2024&#34;&gt;&#xA;    &lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;  &lt;div class=&#34;publication-info&#34;&gt;&#xA;    &lt;h3 class=&#34;publication-title&#34;&gt;&#xA;      &lt;a href=&#34;https://www.biorxiv.org/content/biorxiv/early/2024/11/21/2024.06.24.600505.full.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Bilingual language processing relies on shared semantic representations that are modulated by each language (Chen et al., bioRxiv preprint)&lt;/a&gt;&#xA;    &lt;/h3&gt;&#xA;    &lt;p class=&#34;publication-date&#34;&gt;2024-11-21&lt;/p&gt;</description>
    </item>
    <item>
      <title>Brain Viewers</title>
      <link>http://localhost:4001/brain-viewers/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:4001/brain-viewers/</guid>
      <description>&lt;p&gt;This page collects public brain viewers that you can use to interact with the data and results from many of our published studies. To reach the brain viewer for any topic, just click on the highlighted hyperlink. Please note that these brain viewers do not run well on cell phones, you will have the best experience with a computer or a tablet.&lt;/p&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;div class=&#34;publication-card card fade-in&#34;&gt;&#xA;  &lt;div class=&#34;publication-image&#34;&gt;&#xA;    &lt;a href=&#34;https://blsemc.github.io/viewer/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&#xA;      &lt;img src=&#34;http://localhost:4001/img/papers/Chen.etal.2024.2.webp&#34; alt=&#34;Chen bilingual 2024&#34;&gt;&#xA;    &lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;  &lt;div class=&#34;publication-info&#34;&gt;&#xA;    &lt;h3 class=&#34;publication-title&#34;&gt;&#xA;      &lt;a href=&#34;https://blsemc.github.io/viewer/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Bilingual language processing relies on shared semantic representations that are modulated by each language (Chen et al., bioRxiv preprint, 2024)&lt;/a&gt;&#xA;    &lt;/h3&gt;&#xA;    &lt;div class=&#34;publication-description&#34;&gt;Billions of people throughout the world are bilingual and can extract meaning from multiple languages. To determine how semantic representations in the brains of bilinguals can support both shared and distinct processing for different languages, we performed fMRI scans of participants who are fluent in both English and Chinese while they read natural narratives in each language. This brain viewer allows you to explore, compare and contrast English and Chinese semantic representations in one bilingual participant.&lt;/div&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;div class=&#34;publication-card card fade-in&#34;&gt;&#xA;  &lt;div class=&#34;publication-image&#34;&gt;&#xA;    &lt;a href=&#34;https://gallantlab.org/viewer-chen-2024/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&#xA;      &lt;img src=&#34;http://localhost:4001/img/papers/Chen.etal.2024.webp&#34; alt=&#34;Chen 2024&#34;&gt;&#xA;    &lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;  &lt;div class=&#34;publication-info&#34;&gt;&#xA;    &lt;h3 class=&#34;publication-title&#34;&gt;&#xA;      &lt;a href=&#34;https://gallantlab.org/viewer-chen-2024/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;The cortical representation of language timescales is shared between reading and listening (Chen et al., Communications Biology, 2024)&lt;/a&gt;&#xA;    &lt;/h3&gt;&#xA;    &lt;div class=&#34;publication-description&#34;&gt;Language comprehension involves integrating low-level sensory inputs into a hierarchy of increasingly high-level features. To recover this hierarchy we mapped the intrinsic timescale of language representation across the cerebral cortex during listening and reading. We find that the timescale of representation is organized similarly for the two modalities. The interactive brain viewer shows how the timescales of language representation change systematically across the cortical surface. The colors on the cortical map indicate the context length for language representation.&lt;/div&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;div class=&#34;publication-card card fade-in&#34;&gt;&#xA;  &lt;div class=&#34;publication-image&#34;&gt;&#xA;    &lt;a href=&#34;https://gallantlab.org/viewer-deniz-2019/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&#xA;      &lt;img src=&#34;http://localhost:4001/img/papers/Deniz.F.2019.webp&#34; alt=&#34;Deniz 2019&#34;&gt;&#xA;    &lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;  &lt;div class=&#34;publication-info&#34;&gt;&#xA;    &lt;h3 class=&#34;publication-title&#34;&gt;&#xA;      &lt;a href=&#34;https://gallantlab.org/viewer-deniz-2019/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;The representation of semantic information across human cerebral cortex during listening versus reading is invariant to stimulus modality (Deniz et al., J. Neuroscience, 2019)&lt;/a&gt;&#xA;    &lt;/h3&gt;&#xA;    &lt;div class=&#34;publication-description&#34;&gt;In this experiment, people listened to and read stories from the Moth Radio Hour while brain activity was recorded. Voxelwise modeling was used to determine how each individual brain location responded to semantic concepts in the stories during listening and reading, separately. The interactive brain viewer shows how these concepts are mapped across the cortical surface for both modalities (listening and reading). The colors on the cortical map indicate the semantic concepts that will elicit brain activity at that location during listening and reading.&lt;/div&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;div class=&#34;publication-card card fade-in&#34;&gt;&#xA;  &lt;div class=&#34;publication-image&#34;&gt;&#xA;    &lt;a href=&#34;https://gallantlab.org/viewer-lescroart-2018/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&#xA;      &lt;img src=&#34;http://localhost:4001/img/papers/Lescroart.M.2019.webp&#34; alt=&#34;Lescroart 2019&#34;&gt;&#xA;    &lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;  &lt;div class=&#34;publication-info&#34;&gt;&#xA;    &lt;h3 class=&#34;publication-title&#34;&gt;&#xA;      &lt;a href=&#34;https://gallantlab.org/viewer-lescroart-2018/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Human scene-selective areas represent the 3D configuration of surfaces (Lescroart et al., Neuron, 2018)&lt;/a&gt;&#xA;    &lt;/h3&gt;&#xA;    &lt;div class=&#34;publication-description&#34;&gt;In this experiment people viewed rendered animations depicting objects placed in scenes. The MRI data were analyzed by voxelwise modeling to recover the cortical representation of low-level features and 3D structure. This demo shows how surface position, distance and orientation are mapped across the cortical surface.&lt;/div&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;div class=&#34;publication-card card fade-in&#34;&gt;&#xA;  &lt;div class=&#34;publication-image&#34;&gt;&#xA;    &lt;a href=&#34;https://gallantlab.org/viewer-huth-2016/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&#xA;      &lt;img src=&#34;http://localhost:4001/img/papers/Huth.A.2016.webp&#34; alt=&#34;Huth 2016&#34;&gt;&#xA;    &lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;  &lt;div class=&#34;publication-info&#34;&gt;&#xA;    &lt;h3 class=&#34;publication-title&#34;&gt;&#xA;      &lt;a href=&#34;https://gallantlab.org/viewer-huth-2016/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Natural speech reveals the semantic maps that tile human cerebral cortex (Huth et al., Nature, 2016)&lt;/a&gt;&#xA;    &lt;/h3&gt;&#xA;    &lt;div class=&#34;publication-description&#34;&gt;In this experiment people passively listened to stories from the Moth Radio Hour while brain activity was recorded. Voxelwise modeling was used to determine how each individual brain location responded to 985 distinct semantic concepts in the stories. The demo shows how these concepts are mapped across the cortical surface. The colors on the cortical map show indicate the semantic concepts that will elicit brain activity at that location. The word cloud at right shows words that the model predicts would evoke the largest brain response at the indicated location. Follow the tutorial at upper right to find out more about this tool.&lt;/div&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;div class=&#34;publication-card card fade-in&#34;&gt;&#xA;  &lt;div class=&#34;publication-image&#34;&gt;&#xA;    &lt;a href=&#34;https://gallantlab.org/viewer-cukur-2013/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&#xA;      &lt;img src=&#34;http://localhost:4001/img/papers/Cukur.T.2013.webp&#34; alt=&#34;Cukur 2013&#34;&gt;&#xA;    &lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;  &lt;div class=&#34;publication-info&#34;&gt;&#xA;    &lt;h3 class=&#34;publication-title&#34;&gt;&#xA;      &lt;a href=&#34;https://gallantlab.org/viewer-cukur-2013/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Attention during natural vision warps semantic representations across the human brain (Cukur et al., Nature Neuroscience, 2013)&lt;/a&gt;&#xA;    &lt;/h3&gt;&#xA;    &lt;div class=&#34;publication-description&#34;&gt;In this experiment people passively watched movies while monitoring for the presence of either &#34;humans&#34; or &#34;vehicles&#34;, and in a neutral condition. Voxelwise modeling was used to determine how each brain location responded to 985 distinct categories of objects and actions in the movies, and how these responses were modulated by attention. This brain viewer allows you to view data collected under the three different conditions (left click &#34;Passive Viewing&#34;, &#34;Attending to Humans&#34; or &#34;Attending to Vehicles&#34;). By selecting single brain locations (left click on the brain) or single categories (left click on the WordNet tree), you can see how tuning changes under different states of attention.&lt;/div&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;div class=&#34;publication-card card fade-in&#34;&gt;&#xA;  &lt;div class=&#34;publication-image&#34;&gt;&#xA;    &lt;a href=&#34;https://gallantlab.org/viewer-stories-group/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&#xA;      &lt;img src=&#34;http://localhost:4001/img/papers/Huth.A.2016.webp&#34; alt=&#34;Group semantic maps&#34;&gt;&#xA;    &lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;  &lt;div class=&#34;publication-info&#34;&gt;&#xA;    &lt;h3 class=&#34;publication-title&#34;&gt;&#xA;      &lt;a href=&#34;https://gallantlab.org/viewer-stories-group/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Group-based language comprehension semantic map viewer&lt;/a&gt;&#xA;    &lt;/h3&gt;&#xA;    &lt;div class=&#34;publication-description&#34;&gt;In 2016 we &lt;a href=&#39;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4852309/&#39;&gt;published a paper&lt;/a&gt; that used fMRI, a language comprehension experiment, and voxelwise encoding models to map lexical semantic concepts across the cortical surface. We released brain viewer for that study (see above on this page), but that viewer only showed data from one participant. This viewer provides a way to inspect cortical lexical-semantic conceptual maps at the group level, vertex-by-vertex. The data for this viewer were generated by pooling lexical semantic maps from 24 separate participants who listened to several hours of natural narrative stories. Based on the results that we reported in &lt;a href=&#39;https://www.biorxiv.org/content/10.1101/2025.08.22.671848v1&#39;&gt;another recent paper&lt;/a&gt;, this viewer should account for about 80% of the variance in lexical semantic conceptual maps in any individual. &lt;em&gt;Please note that although this viewer is usable, it is still in development. In the coming weeks the viewer interface will improve and more documentation will be provided.&lt;/em&gt;&lt;/div&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;div class=&#34;publication-card card fade-in&#34;&gt;&#xA;  &lt;div class=&#34;publication-image&#34;&gt;&#xA;    &lt;a href=&#34;https://gallantlab.org/viewer-shortclips-group/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&#xA;      &lt;img src=&#34;http://localhost:4001/img/other/viewer.Huth.A.2012.webp&#34; alt=&#34;Group short movie clip semantic maps&#34;&gt;&#xA;    &lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;  &lt;div class=&#34;publication-info&#34;&gt;&#xA;    &lt;h3 class=&#34;publication-title&#34;&gt;&#xA;      &lt;a href=&#34;https://gallantlab.org/viewer-shortclips-group/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Group-based short movie clip semantic map viewer&lt;/a&gt;&#xA;    &lt;/h3&gt;&#xA;    &lt;div class=&#34;publication-description&#34;&gt;In 2012 we &lt;a href=&#39;https://www.cell.com/neuron/fulltext/S0896-6273(12)00934-8&#39;&gt;published a paper&lt;/a&gt; that used fMRI, a short movie clip viewing experiment, and voxelwise encoding models to map visual semantic concepts across the cortical surface. We released brain viewer for that study (see above on this page), but that viewer only showed data from one participant. This viewer provides a way to inspect cortical visual-semantic conceptual maps at the group level, vertex-by-vertex. The data for this viewer were generated by pooling visual semantic maps from 15 separate participants who watched several hours of short movie clips. &lt;em&gt;Please note that although this viewer is usable, it is still in development. In the coming weeks the viewer interface will improve and more documentation will be provided.&lt;/em&gt;&lt;/div&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;div class=&#34;publication-card card fade-in&#34;&gt;&#xA;  &lt;div class=&#34;publication-image&#34;&gt;&#xA;    &lt;a href=&#34;https://gallantlab.org/viewer-cortical-anatomy/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&#xA;      &lt;img src=&#34;http://localhost:4001/img/other/sulcigyri.webp&#34; alt=&#34;Sulci and Gyri&#34;&gt;&#xA;    &lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;  &lt;div class=&#34;publication-info&#34;&gt;&#xA;    &lt;h3 class=&#34;publication-title&#34;&gt;&#xA;      &lt;a href=&#34;https://gallantlab.org/viewer-cortical-anatomy/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Cortical anatomy viewer&lt;/a&gt;&#xA;    &lt;/h3&gt;&#xA;    &lt;div class=&#34;publication-description&#34;&gt;In order to be able to visualize the complete cortical surface, neuroscientists often work with inflated or flattened cortical maps. However, it can be difficult to orient oneself correctly when inspecting these maps. This viewer provides labels for many of the most commonly referenced sulci and gyri. By switching between folded, inflated and flattened views one can get a good sense of how important cortical landmarks vary across these different views.&lt;/div&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;div class=&#34;publication-card card fade-in&#34;&gt;&#xA;  &lt;div class=&#34;publication-image&#34;&gt;&#xA;    &lt;a href=&#34;https://gallantlab.org/viewer-retinotopy-demo/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&#xA;      &lt;img src=&#34;http://localhost:4001/img/other/retinotopy.webp&#34; alt=&#34;Retinotopy&#34;&gt;&#xA;    &lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;  &lt;div class=&#34;publication-info&#34;&gt;&#xA;    &lt;h3 class=&#34;publication-title&#34;&gt;&#xA;      &lt;a href=&#34;https://gallantlab.org/viewer-retinotopy-demo/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Retinotopy viewer&lt;/a&gt;&#xA;    &lt;/h3&gt;&#xA;    &lt;div class=&#34;publication-description&#34;&gt;The human brain contains many different retinotopic maps, and these maps are one of the primary tools used to parcellate the visual system. Given the large number of maps and their complicated spatial relationships to one another, it is often difficult for students to fully understand how the maps are related. This viewer shows real-time functional activity evoked in a retinal mapping experiment. By identifying the angular and eccentricity functional maps one can gain a good understanding of retinotopic organization.&lt;/div&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;</description>
    </item>
    <item>
      <title>Learn</title>
      <link>http://localhost:4001/learn/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:4001/learn/</guid>
      <description>&lt;div class=&#34;publication-card card fade-in&#34;&gt;&#xA;  &lt;div class=&#34;publication-image&#34;&gt;&#xA;    &lt;a href=&#34;https://www.psyarxiv.com/nt2jq&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&#xA;      &lt;img src=&#34;http://localhost:4001/img/papers/ViscontidOC.Deniz.2025.webp&#34; alt=&#34;Voxelwise Encoding Model guide&#34;&gt;&#xA;    &lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;  &lt;div class=&#34;publication-info&#34;&gt;&#xA;    &lt;h3 class=&#34;publication-title&#34;&gt;&#xA;      &lt;a href=&#34;https://www.psyarxiv.com/nt2jq&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Voxelwise Encoding Model review paper&lt;/a&gt;&#xA;    &lt;/h3&gt;&#xA;    &lt;div class=&#34;publication-description&#34;&gt;This review paper provides the first comprehensive guide to the Voxelwise Encoding Model (VEM) framework. The VEM framework is a framework for fitting encoding models to fMRI data. This framework is currently the most sensitive and powerful approach available for modeling fMRI data. It can be used to fit dozens of distinct models simultaneously, each model having up to several thousand distinct features. The Voxelwise Encoding Model framework also conforms to all best practices in data science, which maximizes sensitivity, reliability and generalizability of the resulting models.&lt;/div&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;div class=&#34;publication-card card fade-in&#34;&gt;&#xA;  &lt;div class=&#34;publication-image&#34;&gt;&#xA;    &lt;a href=&#34;https://gallantlab.org/voxelwise_tutorials&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&#xA;      &lt;img src=&#34;http://localhost:4001/img/other/learn.vm.webp&#34; alt=&#34;Voxelwise Encoding Model tutorials&#34;&gt;&#xA;    &lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;  &lt;div class=&#34;publication-info&#34;&gt;&#xA;    &lt;h3 class=&#34;publication-title&#34;&gt;&#xA;      &lt;a href=&#34;https://gallantlab.org/voxelwise_tutorials&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Voxelwise Encoding Model tutorials&lt;/a&gt;&#xA;    &lt;/h3&gt;&#xA;    &lt;div class=&#34;publication-description&#34;&gt;These Python tutorials show how to fit, evaluate, and interpret voxelwise encoding models on one of our public available datasets. We are providing these online tutorials here as a service to the fMRI community.&lt;/div&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;div class=&#34;publication-card card fade-in&#34;&gt;&#xA;  &lt;div class=&#34;publication-image&#34;&gt;&#xA;    &lt;a href=&#34;https://www.youtube.com/watch?v=jobQmEJpbhY&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&#xA;      &lt;img src=&#34;http://localhost:4001/img/other/learn.workshop.webp&#34; alt=&#34;Voxelwise Encoding Model CCN workshop&#34;&gt;&#xA;    &lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;  &lt;div class=&#34;publication-info&#34;&gt;&#xA;    &lt;h3 class=&#34;publication-title&#34;&gt;&#xA;      &lt;a href=&#34;https://www.youtube.com/watch?v=jobQmEJpbhY&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Voxelwise Encoding Model workshop video&lt;/a&gt;&#xA;    &lt;/h3&gt;&#xA;    &lt;div class=&#34;publication-description&#34;&gt;At the 2021 CCN meeting we held a keynote and tutorials session on the Voxelwise Encoding Model framework. You can find a video recording of the workshop here.&lt;/div&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;</description>
    </item>
    <item>
      <title>Code</title>
      <link>http://localhost:4001/code/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:4001/code/</guid>
      <description>&lt;div class=&#34;publication-card card fade-in&#34;&gt;&#xA;  &lt;div class=&#34;publication-image&#34;&gt;&#xA;    &lt;a href=&#34;https://github.com/gallantlab&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&#xA;      &lt;img src=&#34;http://localhost:4001/img/other/github.webp&#34; alt=&#34;github&#34;&gt;&#xA;    &lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;  &lt;div class=&#34;publication-info&#34;&gt;&#xA;    &lt;h3 class=&#34;publication-title&#34;&gt;&#xA;      &lt;a href=&#34;https://github.com/gallantlab&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;GitHub repository&lt;/a&gt;&#xA;    &lt;/h3&gt;&#xA;    &lt;div class=&#34;publication-description&#34;&gt;Our github repository contains lots of open code that may be useful for scientific computing in general, or for fitting encoding models to fMRI or neurophysiology data sets.&lt;/div&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;div class=&#34;publication-card card fade-in&#34;&gt;&#xA;  &lt;div class=&#34;publication-image&#34;&gt;&#xA;    &lt;a href=&#34;https://github.com/gallantlab/himalaya&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&#xA;      &lt;img src=&#34;http://localhost:4001/img/other/himalaya.webp&#34; alt=&#34;himalaya&#34;&gt;&#xA;    &lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;  &lt;div class=&#34;publication-info&#34;&gt;&#xA;    &lt;h3 class=&#34;publication-title&#34;&gt;&#xA;      &lt;a href=&#34;https://github.com/gallantlab/himalaya&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Himalaya&lt;/a&gt;&#xA;    &lt;/h3&gt;&#xA;    &lt;div class=&#34;publication-description&#34;&gt;Himalaya implements machine learning linear(ized) models in Python, focusing on computational efficiency for large numbers of targets. Himalaya efficiently estimates linear(ized) models on large numbers of targets (for example, thousands of voxels in an fMRI experiment), it runs on both CPU and GPU hardware, and it provides estimators that are compatible with scikit-learn&#39;s API. Himalaya is routinely used in our lab to fit voxelwise encoding models to very large fMRI data sets. A paper describing Himalaya can be found &lt;a href=&#39;https://www.sciencedirect.com/science/article/pii/S1053811922008497&#39;&gt;here&lt;/a&gt;.&lt;/div&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;div class=&#34;publication-card card fade-in&#34;&gt;&#xA;  &lt;div class=&#34;publication-image&#34;&gt;&#xA;    &lt;a href=&#34;https://github.com/gallantlab/pycortex&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&#xA;      &lt;img src=&#34;http://localhost:4001/img/other/pycortex.webp&#34; alt=&#34;pycortex&#34;&gt;&#xA;    &lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;  &lt;div class=&#34;publication-info&#34;&gt;&#xA;    &lt;h3 class=&#34;publication-title&#34;&gt;&#xA;      &lt;a href=&#34;https://github.com/gallantlab/pycortex&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Pycortex&lt;/a&gt;&#xA;    &lt;/h3&gt;&#xA;    &lt;div class=&#34;publication-description&#34;&gt;Pycortex is a python-based toolkit for surface visualization of fMRI data. (It can also be used to visualize other types of volumetric brain data.) The brain viewers on this site were all generated using Pycortex. The 2015 publication describing Pycortex can be found &lt;a href=&#39;https://www.frontiersin.org/articles/10.3389/fninf.2015.00023/full&#39;&gt;here&lt;/a&gt;. Documentation for Pycortex can be found &lt;a href=&#39;https://gallantlab.github.io/pycortex/&#39;&gt;here&lt;/a&gt;.&lt;/div&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;div class=&#34;publication-card card fade-in&#34;&gt;&#xA;  &lt;div class=&#34;publication-image&#34;&gt;&#xA;    &lt;a href=&#34;https://github.com/gallantlab/cottoncandy&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&#xA;      &lt;img src=&#34;http://localhost:4001/img/other/cottoncandy.webp&#34; alt=&#34;cottoncandy&#34;&gt;&#xA;    &lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;  &lt;div class=&#34;publication-info&#34;&gt;&#xA;    &lt;h3 class=&#34;publication-title&#34;&gt;&#xA;      &lt;a href=&#34;https://github.com/gallantlab/cottoncandy&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;CottonCandy&lt;/a&gt;&#xA;    &lt;/h3&gt;&#xA;    &lt;div class=&#34;publication-description&#34;&gt;CottonCandy is a scientific library for storing and accessing numpy array data on an S3-compatible cloud storage instance. This is achieved by reading arrays from memory and downloading arrays directly into memory. This means that you don&#39;t have to download your array to disk, and then load it from disk into your python session. A paper describing CottonCandy can be found &lt;a href=&#39;https://joss.theoj.org/papers/10.21105/joss.00890.pdf&#39;&gt;here&lt;/a&gt;.&lt;/div&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;</description>
    </item>
    <item>
      <title>Data</title>
      <link>http://localhost:4001/data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:4001/data/</guid>
      <description>&lt;div class=&#34;publication-card card fade-in&#34;&gt;&#xA;  &lt;div class=&#34;publication-image&#34;&gt;&#xA;    &lt;a href=&#34;https://doi.gin.g-node.org/10.12751/g-node.vy1zjd/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&#xA;      &lt;img src=&#34;http://localhost:4001/img/datasets/Popham.etal.2021.webp&#34; alt=&#34;Gallant Lab Natural Short Clips fMRI Dataset&#34;&gt;&#xA;    &lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;  &lt;div class=&#34;publication-info&#34;&gt;&#xA;    &lt;h3 class=&#34;publication-title&#34;&gt;&#xA;      &lt;a href=&#34;https://doi.gin.g-node.org/10.12751/g-node.vy1zjd/&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Natural Short Clips 3T fMRI Data&lt;/a&gt;&#xA;    &lt;/h3&gt;&#xA;    &lt;div class=&#34;publication-description&#34;&gt;This dataset contains BOLD fMRI responses from 5 human subjects viewing natural short video clips across 3 sessions over 3 separate days for each subject. This dataset has been used in multiple publications studying visual processing and semantic representation. If you publish work that uses these data please cite the following paper: &lt;a href=&#39;https://doi.org/10.1038/s41593-021-00921-6&#39;&gt;Popham, S. F., Huth, A. G., Bilenko, N. Y., Deniz, F., Gao, J. S., Nunez-Elizalde, A. O., &amp; Gallant, J. L. (2021). Visual and linguistic semantic representations are aligned at the border of human visual cortex. Nature Neuroscience, 24(11), 1628-1636.&lt;/a&gt;&lt;/div&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;div class=&#34;publication-card card fade-in&#34;&gt;&#xA;  &lt;div class=&#34;publication-image&#34;&gt;&#xA;    &lt;a href=&#34;http://crcns.org/data-sets/vc/vim-4/about-vim-4&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&#xA;      &lt;img src=&#34;http://localhost:4001/img/datasets/Zhang.etal.2021.webp&#34; alt=&#34;Gallant Lab Naturalistic fMRI Dataset&#34;&gt;&#xA;    &lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;  &lt;div class=&#34;publication-info&#34;&gt;&#xA;    &lt;h3 class=&#34;publication-title&#34;&gt;&#xA;      &lt;a href=&#34;http://crcns.org/data-sets/vc/vim-4/about-vim-4&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Naturalistic fMRI Data (vim-4)&lt;/a&gt;&#xA;    &lt;/h3&gt;&#xA;    &lt;div class=&#34;publication-description&#34;&gt;This dataset contains whole-brain BOLD fMRI responses from human subjects performing two tasks: a visual attention task (passive movie watching with attention directed to humans or vehicles) and a video game task (open-ended Counterstrike gameplay). This dataset enables analysis of task-related cognitive states in naturalistic experimental conditions. If you publish work that uses these data please cite the following paper: &lt;a href=&#39;https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2020.565976/full&#39;&gt;Zhang, T., Gao, J. S., Çukur, T., &amp; Gallant, J. L. (2021). Voxel-based state space modeling recovers task-related cognitive states in naturalistic fmri experiments. Frontiers in neuroscience, 14, 565976.&lt;/a&gt;&lt;/div&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;div class=&#34;publication-card card fade-in&#34;&gt;&#xA;  &lt;div class=&#34;publication-image&#34;&gt;&#xA;    &lt;a href=&#34;https://gin.g-node.org/denizenslab/narratives_reading_listening_fmri/src/master/chen2024_timescales&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&#xA;      &lt;img src=&#34;http://localhost:4001/img/datasets/Deniz.etal.2019.webp&#34; alt=&#34;Gallant Lab Semantic Listening vs Reading fMRI Dataset&#34;&gt;&#xA;    &lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;  &lt;div class=&#34;publication-info&#34;&gt;&#xA;    &lt;h3 class=&#34;publication-title&#34;&gt;&#xA;      &lt;a href=&#34;https://gin.g-node.org/denizenslab/narratives_reading_listening_fmri/src/master/chen2024_timescales&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Semantic Listening vs Reading fMRI Data&lt;/a&gt;&#xA;    &lt;/h3&gt;&#xA;    &lt;div class=&#34;publication-description&#34;&gt;This dataset contains BOLD fMRI responses from human subjects during listening and reading tasks, demonstrating that semantic information representation across cerebral cortex remains invariant to stimulus modality. This dataset enables investigation of cross-modal semantic processing in the brain. If you publish work that uses these data please cite the following paper: &lt;a href=&#39;https://www.jneurosci.org/content/39/39/7722&#39;&gt;Deniz, F., Nunez-Elizalde, A. O., Huth, A. G., &amp; Gallant, J. L. (2019). The representation of semantic information across human cerebral cortex during listening versus reading is invariant to stimulus modality. Journal of Neuroscience, 39(39), 7722-7736.&lt;/a&gt;&lt;/div&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;div class=&#34;publication-card card fade-in&#34;&gt;&#xA;  &lt;div class=&#34;publication-image&#34;&gt;&#xA;    &lt;a href=&#34;https://gin.g-node.org/gallantlab/story_listening&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&#xA;      &lt;img src=&#34;http://localhost:4001/img/papers/Huth.A.2016.webp&#34; alt=&#34;Gallant Lab Story Listening fMRI Dataset&#34;&gt;&#xA;    &lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;  &lt;div class=&#34;publication-info&#34;&gt;&#xA;    &lt;h3 class=&#34;publication-title&#34;&gt;&#xA;      &lt;a href=&#34;https://gin.g-node.org/gallantlab/story_listening&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Story Listening fMRI Data&lt;/a&gt;&#xA;    &lt;/h3&gt;&#xA;    &lt;div class=&#34;publication-description&#34;&gt;This dataset contains BOLD fMRI responses from human subjects listening to natural narrative stories. This dataset was used to map semantic representations across the human cerebral cortex and create detailed semantic atlases of language processing. If you publish work that uses these data please cite the following paper: &lt;a href=&#39;https://www.nature.com/articles/nature17637&#39;&gt;Huth, A. G., De Heer, W. A., Griffiths, T. L., Theunissen, F. E., &amp; Gallant, J. L. (2016). Natural speech reveals the semantic maps that tile human cerebral cortex. Nature, 532(7600), 453-458.&lt;/a&gt;&lt;/div&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;div class=&#34;publication-card card fade-in&#34;&gt;&#xA;  &lt;div class=&#34;publication-image&#34;&gt;&#xA;    &lt;a href=&#34;https://crcns.org/data-sets/vc/vim-2/about-vim-2&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&#xA;      &lt;img src=&#34;http://localhost:4001/img/datasets/Nishimoto.etal.2011.webp&#34; alt=&#34;Gallant Lab Natural Movie fMRI Dataset&#34;&gt;&#xA;    &lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;  &lt;div class=&#34;publication-info&#34;&gt;&#xA;    &lt;h3 class=&#34;publication-title&#34;&gt;&#xA;      &lt;a href=&#34;https://crcns.org/data-sets/vc/vim-2/about-vim-2&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Natural Movie 4T fMRI Data (vim-2)&lt;/a&gt;&#xA;    &lt;/h3&gt;&#xA;    &lt;div class=&#34;publication-description&#34;&gt;This dataset contains BOLD fMRI responses from 3 subjects viewing natural movies, collected across 3 sessions on separate days. This dataset was used in the landmark study reconstructing visual experiences from brain activity. If you publish work that uses these data please cite the following paper: &lt;a href=&#39;https://www.sciencedirect.com/science/article/pii/S0960982211009377&#39;&gt;Nishimoto, S., Vu, A. T., Naselaris, T., Benjamini, Y., Yu, B., &amp; Gallant, J. L. (2011). Reconstructing visual experiences from brain activity evoked by natural movies. Current Biology, 21(19), 1641-1646.&lt;/a&gt;&lt;/div&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&#xA;&lt;div class=&#34;publication-card card fade-in&#34;&gt;&#xA;  &lt;div class=&#34;publication-image&#34;&gt;&#xA;    &lt;a href=&#34;http://crcns.org/data-sets/vc/vim-1/about-vim-1&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;&#xA;      &lt;img src=&#34;http://localhost:4001/img/datasets/Kay.etal.2008.webp&#34; alt=&#34;Gallant Lab Natural Images fMRI Dataset&#34;&gt;&#xA;    &lt;/a&gt;&#xA;  &lt;/div&gt;&#xA;  &lt;div class=&#34;publication-info&#34;&gt;&#xA;    &lt;h3 class=&#34;publication-title&#34;&gt;&#xA;      &lt;a href=&#34;http://crcns.org/data-sets/vc/vim-1/about-vim-1&#34; target=&#34;_blank&#34; rel=&#34;noopener noreferrer&#34;&gt;Natural Images fMRI Data (vim-1)&lt;/a&gt;&#xA;    &lt;/h3&gt;&#xA;    &lt;div class=&#34;publication-description&#34;&gt;This dataset contains BOLD fMRI responses from 2 subjects viewing natural images across 70 experimental runs collected over 5 separate days. This dataset demonstrated the ability to identify which images a subject was viewing from brain activity using Bayesian reconstruction techniques. If you publish work that uses these data please cite the following paper: &lt;a href=&#39;https://www.nature.com/articles/nature06713&#39;&gt;Kay, K. N., Naselaris, T., Prenger, R. J., &amp; Gallant, J. L. (2008). Identifying natural images from human brain activity. Nature, 452(7185), 352-355.&lt;/a&gt;&lt;/div&gt;&#xA;  &lt;/div&gt;&#xA;&lt;/div&gt;</description>
    </item>
    <item>
      <title>Join Us</title>
      <link>http://localhost:4001/joinus/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:4001/joinus/</guid>
      <description>&lt;h1 id=&#34;general-guidance-on-recruitment&#34;&gt;General guidance on recruitment&lt;/h1&gt;&#xA;&lt;p&gt;Our laboratory is very interdisciplinary, and we are always on the lookout for good postdocs and graduate students with a strong desire to break new ground in cognitive, systems and computational neuroscience. Prior areas of training are not particularly important; training in neuroscience, neuroimaging, and data science tailored for neuroscience are typically part of graduate and postdoctoral training in the lab. That said, strong quantitative and computational skills are an asset.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Blog</title>
      <link>http://localhost:4001/blog/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:4001/blog/</guid>
      <description>&lt;h2 id=&#34;blog-posts&#34;&gt;Blog Posts&lt;/h2&gt;&#xA;&lt;p&gt;{{ range (where .Site.RegularPages &amp;ldquo;Section&amp;rdquo; &amp;ldquo;news&amp;rdquo;).ByDate.Reverse }}&lt;/p&gt;&#xA;&lt;!-- raw HTML omitted --&gt;</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:4001/news/2025-09-17-vem-framework-paper/</link>
      <pubDate>Wed, 17 Sep 2025 12:00:00 -0800</pubDate>
      <guid>http://localhost:4001/news/2025-09-17-vem-framework-paper/</guid>
      <description></description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:4001/news/2025-09-15-group-shortclip-brain-viewer/</link>
      <pubDate>Mon, 15 Sep 2025 12:00:00 -0800</pubDate>
      <guid>http://localhost:4001/news/2025-09-15-group-shortclip-brain-viewer/</guid>
      <description></description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:4001/news/2025-09-05-group-brain-viewer/</link>
      <pubDate>Fri, 05 Sep 2025 12:00:00 -0800</pubDate>
      <guid>http://localhost:4001/news/2025-09-05-group-brain-viewer/</guid>
      <description></description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:4001/news/2025-08-23-visconti-bioarxiv/</link>
      <pubDate>Sat, 23 Aug 2025 12:00:00 -0800</pubDate>
      <guid>http://localhost:4001/news/2025-08-23-visconti-bioarxiv/</guid>
      <description></description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:4001/news/2025-05-15-emily-meschke-phd/</link>
      <pubDate>Thu, 15 May 2025 12:00:00 -0800</pubDate>
      <guid>http://localhost:4001/news/2025-05-15-emily-meschke-phd/</guid>
      <description></description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:4001/news/2025-05-09-voxelwise-encoding-tutorial/</link>
      <pubDate>Fri, 09 May 2025 12:00:00 -0800</pubDate>
      <guid>http://localhost:4001/news/2025-05-09-voxelwise-encoding-tutorial/</guid>
      <description></description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:4001/news/2024-11-01-evi-hendrikx-joins/</link>
      <pubDate>Fri, 01 Nov 2024 12:00:00 -0800</pubDate>
      <guid>http://localhost:4001/news/2024-11-01-evi-hendrikx-joins/</guid>
      <description></description>
    </item>
    <item>
      <title>New paper from Deniz et al. on how semantic representations during language production are affected by context, published in J. Neuroscience</title>
      <link>http://localhost:4001/news/2024-08-15-deniz-context-paper/</link>
      <pubDate>Thu, 15 Aug 2024 12:00:00 -0800</pubDate>
      <guid>http://localhost:4001/news/2024-08-15-deniz-context-paper/</guid>
      <description></description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:4001/news/2024-07-01-gong-phonemic-paper/</link>
      <pubDate>Mon, 01 Jul 2024 12:00:00 -0800</pubDate>
      <guid>http://localhost:4001/news/2024-07-01-gong-phonemic-paper/</guid>
      <description></description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:4001/news/2024-05-15-catherine-chen-phd/</link>
      <pubDate>Wed, 15 May 2024 12:00:00 -0800</pubDate>
      <guid>http://localhost:4001/news/2024-05-15-catherine-chen-phd/</guid>
      <description></description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:4001/news/2024-04-01-fatma-professor-position/</link>
      <pubDate>Mon, 01 Apr 2024 12:00:00 -0800</pubDate>
      <guid>http://localhost:4001/news/2024-04-01-fatma-professor-position/</guid>
      <description></description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:4001/news/2023-06-01-meschke-preprint/</link>
      <pubDate>Thu, 01 Jun 2023 12:00:00 -0800</pubDate>
      <guid>http://localhost:4001/news/2023-06-01-meschke-preprint/</guid>
      <description></description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:4001/news/2022-08-01-dupre-banded-ridge/</link>
      <pubDate>Mon, 01 Aug 2022 12:00:00 -0800</pubDate>
      <guid>http://localhost:4001/news/2022-08-01-dupre-banded-ridge/</guid>
      <description></description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:4001/news/2022-05-01-christine-tseng-phd/</link>
      <pubDate>Sun, 01 May 2022 12:00:00 -0800</pubDate>
      <guid>http://localhost:4001/news/2022-05-01-christine-tseng-phd/</guid>
      <description></description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:4001/news/2021-11-20-popham-nature-neuroscience/</link>
      <pubDate>Sat, 20 Nov 2021 12:00:00 -0800</pubDate>
      <guid>http://localhost:4001/news/2021-11-20-popham-nature-neuroscience/</guid>
      <description></description>
    </item>
  </channel>
</rss>
