<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>Brain Viewers | Gallant Lab</title>
<meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Interactive brain viewers from Gallant Lab publications"><meta name=generator content="Hugo 0.139.3"><meta name=robots content="index, follow"><link rel=stylesheet href=/ananke/css/main.min.d05fb5f317fcf33b3a52936399bdf6f47dc776516e1692e412ec7d76f4a5faa2.css><link rel=canonical href=https://gallantlab.org/brain-viewers/><meta property="og:url" content="https://gallantlab.org/brain-viewers/"><meta property="og:site_name" content="Gallant Lab"><meta property="og:title" content="Brain Viewers"><meta property="og:description" content="Interactive brain viewers from Gallant Lab publications"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta itemprop=name content="Brain Viewers"><meta itemprop=description content="Interactive brain viewers from Gallant Lab publications"><meta itemprop=wordCount content="1098"><meta name=twitter:card content="summary"><meta name=twitter:title content="Brain Viewers"><meta name=twitter:description content="Interactive brain viewers from Gallant Lab publications"><link rel=stylesheet href=/css/custom.min.css><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel=stylesheet></head><body class="ma0 avenir bg-near-white production"><header><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l center items-center justify-between"><div class="flex-l items-center w-100"><a href=/ class="f3 fw7 hover-white white no-underline" style=margin-right:2rem>Gallant Lab</a><ul class="pl0 mr3"><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/ title="About page">About</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/people/ title="People page">People</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/publications/ title="Publications page">Publications</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/brain-viewers/ title="Brain Viewers page">Brain Viewers</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/learn/ title="Learn page">Learn</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/code/ title="Code page">Code</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/data/ title="Data page">Data</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/joinus/ title="Join Us page">Join Us</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/blog/ title="Blog page">Blog</a></li></ul><div class=ananke-socials></div></div></div></nav></header><main class=pb7 role=main><div class="flex-l mt2 mw8 center"><article class="center cf pv5 ph3 ph4-ns mw7"><header><h1 class=f1>Brain Viewers</h1></header><div class="nested-copy-line-height lh-copy f4 nested-links mid-gray"><h2 id=brain-viewers>Brain Viewers</h2><p>This page collects public brain viewers that you can use to interact with the data and results from many of our published studies. To reach the brain viewer for any topic, just click on the highlighted hyperlink. Please note that these brain viewers do not run well on cell phones, you will have the best experience with a computer or a tablet.</p><div class="publication-card card fade-in"><div class=publication-image><a href=https://blsemc.github.io/viewer/ target=_blank rel="noopener noreferrer"><img src=/img/papers/Chen.etal.2024.2.webp alt="Chen bilingual 2024"></a></div><div class=publication-info><h3 class=publication-title><a href=https://blsemc.github.io/viewer/ target=_blank rel="noopener noreferrer">Bilingual language processing relies on shared semantic representations that are modulated by each language (Chen et al., bioRxiv preprint, 2024)</a></h3><div class=publication-description>Billions of people throughout the world are bilingual and can extract meaning from multiple languages. To determine how semantic representations in the brains of bilinguals can support both shared and distinct processing for different languages, we performed fMRI scans of participants who are fluent in both English and Chinese while they read natural narratives in each language. This brain viewer allows you to explore, compare and contrast English and Chinese semantic representations in one bilingual participant.</div></div></div><div class="publication-card card fade-in"><div class=publication-image><a href=https://gallantlab.org/viewer-chen-2024/ target=_blank rel="noopener noreferrer"><img src=/img/papers/Chen.etal.2024.webp alt="Chen 2024"></a></div><div class=publication-info><h3 class=publication-title><a href=https://gallantlab.org/viewer-chen-2024/ target=_blank rel="noopener noreferrer">The cortical representation of language timescales is shared between reading and listening (Chen et al., Communications Biology, 2024)</a></h3><div class=publication-description>Language comprehension involves integrating low-level sensory inputs into a hierarchy of increasingly high-level features. To recover this hierarchy we mapped the intrinsic timescale of language representation across the cerebral cortex during listening and reading. We find that the timescale of representation is organized similarly for the two modalities. The interactive brain viewer shows how the timescales of language representation change systematically across the cortical surface. The colors on the cortical map indicate the context length for language representation.</div></div></div><div class="publication-card card fade-in"><div class=publication-image><a href=https://gallantlab.org/viewer-deniz-2019/ target=_blank rel="noopener noreferrer"><img src=/img/papers/Deniz.F.2019.webp alt="Deniz 2019"></a></div><div class=publication-info><h3 class=publication-title><a href=https://gallantlab.org/viewer-deniz-2019/ target=_blank rel="noopener noreferrer">The representation of semantic information across human cerebral cortex during listening versus reading is invariant to stimulus modality (Deniz et al., J. Neuroscience, 2019)</a></h3><div class=publication-description>In this experiment, people listened to and read stories from the Moth Radio Hour while brain activity was recorded. Voxelwise modeling was used to determine how each individual brain location responded to semantic concepts in the stories during listening and reading, separately. The interactive brain viewer shows how these concepts are mapped across the cortical surface for both modalities (listening and reading). The colors on the cortical map indicate the semantic concepts that will elicit brain activity at that location during listening and reading.</div></div></div><div class="publication-card card fade-in"><div class=publication-image><a href=https://gallantlab.org/viewer-lescroart-2018/ target=_blank rel="noopener noreferrer"><img src=/img/papers/Lescroart.M.2019.webp alt="Lescroart 2019"></a></div><div class=publication-info><h3 class=publication-title><a href=https://gallantlab.org/viewer-lescroart-2018/ target=_blank rel="noopener noreferrer">Human scene-selective areas represent the 3D configuration of surfaces (Lescroart et al., Neuron, 2018)</a></h3><div class=publication-description>In this experiment people viewed rendered animations depicting objects placed in scenes. The MRI data were analyzed by voxelwise modeling to recover the cortical representation of low-level features and 3D structure. This demo shows how surface position, distance and orientation are mapped across the cortical surface.</div></div></div><div class="publication-card card fade-in"><div class=publication-image><a href=https://gallantlab.org/viewer-huth-2016/ target=_blank rel="noopener noreferrer"><img src=/img/papers/Huth.A.2016.webp alt="Huth 2016"></a></div><div class=publication-info><h3 class=publication-title><a href=https://gallantlab.org/viewer-huth-2016/ target=_blank rel="noopener noreferrer">Natural speech reveals the semantic maps that tile human cerebral cortex (Huth et al., Nature, 2016)</a></h3><div class=publication-description>In this experiment people passively listened to stories from the Moth Radio Hour while brain activity was recorded. Voxelwise modeling was used to determine how each individual brain location responded to 985 distinct semantic concepts in the stories. The demo shows how these concepts are mapped across the cortical surface. The colors on the cortical map show indicate the semantic concepts that will elicit brain activity at that location. The word cloud at right shows words that the model predicts would evoke the largest brain response at the indicated location. Follow the tutorial at upper right to find out more about this tool.</div></div></div><div class="publication-card card fade-in"><div class=publication-image><a href=https://gallantlab.org/viewer-cukur-2013/ target=_blank rel="noopener noreferrer"><img src=/img/papers/Cukur.T.2013.webp alt="Cukur 2013"></a></div><div class=publication-info><h3 class=publication-title><a href=https://gallantlab.org/viewer-cukur-2013/ target=_blank rel="noopener noreferrer">Attention during natural vision warps semantic representations across the human brain (Cukur et al., Nature Neuroscience, 2013)</a></h3><div class=publication-description>In this experiment people passively watched movies while monitoring for the presence of either "humans" or "vehicles", and in a neutral condition. Voxelwise modeling was used to determine how each brain location responded to 985 distinct categories of objects and actions in the movies, and how these responses were modulated by attention. This brain viewer allows you to view data collected under the three different conditions (left click "Passive Viewing", "Attending to Humans" or "Attending to Vehicles"). By selecting single brain locations (left click on the brain) or single categories (left click on the WordNet tree), you can see how tuning changes under different states of attention.</div></div></div><div class="publication-card card fade-in"><div class=publication-image><a href=https://gallantlab.org/viewer-stories-group/ target=_blank rel="noopener noreferrer"><img src=/img/papers/Huth.A.2016.webp alt="Group semantic maps"></a></div><div class=publication-info><h3 class=publication-title><a href=https://gallantlab.org/viewer-stories-group/ target=_blank rel="noopener noreferrer">Group-based language comprehension semantic map viewer</a></h3><div class=publication-description>In 2016 we <a href=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4852309/>published a paper</a> that used fMRI, a language comprehension experiment, and voxelwise encoding models to map lexical semantic concepts across the cortical surface. We released brain viewer for that study (see above on this page), but that viewer only showed data from one participant. This viewer provides a way to inspect cortical lexical-semantic conceptual maps at the group level, vertex-by-vertex. The data for this viewer were generated by pooling lexical semantic maps from 24 separate participants who listened to several hours of natural narrative stories. Based on the results that we reported in <a href=https://www.biorxiv.org/content/10.1101/2025.08.22.671848v1>another recent paper</a>, this viewer should account for about 80% of the variance in lexical semantic conceptual maps in any individual. <em>Please note that although this viewer is usable, it is still in development. In the coming weeks the viewer interface will improve and more documentation will be provided.</em></div></div></div><div class="publication-card card fade-in"><div class=publication-image><a href=https://gallantlab.org/viewer-shortclips-group/ target=_blank rel="noopener noreferrer"><img src=/img/other/viewer.Huth.A.2012.webp alt="Group short movie clip semantic maps"></a></div><div class=publication-info><h3 class=publication-title><a href=https://gallantlab.org/viewer-shortclips-group/ target=_blank rel="noopener noreferrer">Group-based short movie clip semantic map viewer</a></h3><div class=publication-description>In 2012 we <a href=https://www.cell.com/neuron/fulltext/S0896-6273(12)00934-8>published a paper</a> that used fMRI, a short movie clip viewing experiment, and voxelwise encoding models to map visual semantic concepts across the cortical surface. We released brain viewer for that study (see above on this page), but that viewer only showed data from one participant. This viewer provides a way to inspect cortical visual-semantic conceptual maps at the group level, vertex-by-vertex. The data for this viewer were generated by pooling visual semantic maps from 15 separate participants who watched several hours of short movie clips. <em>Please note that although this viewer is usable, it is still in development. In the coming weeks the viewer interface will improve and more documentation will be provided.</em></div></div></div><div class="publication-card card fade-in"><div class=publication-image><a href=https://gallantlab.org/viewer-cortical-anatomy/ target=_blank rel="noopener noreferrer"><img src=/img/other/sulcigyri.webp alt="Sulci and Gyri"></a></div><div class=publication-info><h3 class=publication-title><a href=https://gallantlab.org/viewer-cortical-anatomy/ target=_blank rel="noopener noreferrer">Cortical anatomy viewer</a></h3><div class=publication-description>In order to be able to visualize the complete cortical surface, neuroscientists often work with inflated or flattened cortical maps. However, it can be difficult to orient oneself correctly when inspecting these maps. This viewer provides labels for many of the most commonly referenced sulci and gyri. By switching between folded, inflated and flattened views one can get a good sense of how important cortical landmarks vary across these different views.</div></div></div><div class="publication-card card fade-in"><div class=publication-image><a href=https://gallantlab.org/viewer-retinotopy-demo/ target=_blank rel="noopener noreferrer"><img src=/img/other/retinotopy.webp alt=Retinotopy></a></div><div class=publication-info><h3 class=publication-title><a href=https://gallantlab.org/viewer-retinotopy-demo/ target=_blank rel="noopener noreferrer">Retinotopy viewer</a></h3><div class=publication-description>The human brain contains many different retinotopic maps, and these maps are one of the primary tools used to parcellate the visual system. Given the large number of maps and their complicated spatial relationships to one another, it is often difficult for students to fully understand how the maps are related. This viewer shows real-time functional activity evoked in a retinal mapping experiment. By identifying the angular and eccentricity functional maps one can gain a good understanding of retinotopic organization.</div></div></div></div></article></div></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-center"><div class="f6 fw4 white-70 tc pv2 ph3">&copy; Copyright 2025 Jack Gallant.
And also by the Regents of the University of California, our benevolent overlords.
Powered by <a href=https://gohugo.io/ target=_blank class="white-70 hover-white">Hugo</a>.
Hosted by <a href=https://pages.github.com/ target=_blank class="white-70 hover-white">GitHub Pages</a>.
Last updated: October 27, 2025.</div></div></footer></body></html>