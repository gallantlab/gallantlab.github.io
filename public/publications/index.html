<!DOCTYPE html>
<html lang="en-us">
  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=4001&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Publications | Gallant Lab</title>
    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="description" content="Gallant Lab Publications">
    <meta name="generator" content="Hugo 0.152.0">
    
    
    
      <meta name="robots" content="noindex, nofollow">
    
    

    
<link rel="stylesheet" href="/ananke/css/main.min.css" >




    


    
      

    

    

    
      <link rel="canonical" href="http://localhost:4001/publications/">
    

    <meta property="og:url" content="http://localhost:4001/publications/">
  <meta property="og:site_name" content="Gallant Lab">
  <meta property="og:title" content="Publications">
  <meta property="og:description" content="Gallant Lab Publications">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">

  <meta itemprop="name" content="Publications">
  <meta itemprop="description" content="Gallant Lab Publications">
  <meta itemprop="wordCount" content="1454">
  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="Publications">
  <meta name="twitter:description" content="Gallant Lab Publications">

	
<link rel="stylesheet" href="/css/custom.min.css" />


<link rel="preconnect" href="https://fonts.googleapis.com" />
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
<link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet" />

  </head><body class="ma0 avenir bg-near-white development">

    
   
  

  <header>
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l center items-center justify-between">
    <div class="flex-l items-center w-100">
      <a href="/" class="f3 fw7 hover-white white no-underline" style="margin-right: 2rem;">
        Gallant Lab
      </a>
      

      
        <ul class="pl0 mr3">
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/" title="About page">
              About
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/people/" title="People page">
              People
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/publications/" title="Publications page">
              Publications
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/brain-viewers/" title="Brain Viewers page">
              Brain Viewers
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/learn/" title="Learn page">
              Learn
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/code/" title="Code page">
              Code
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/data/" title="Data page">
              Data
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/joinus/" title="Join Us page">
              Join Us
            </a>
          </li>
          
          <li class="list f5 f4-ns fw4 dib pr3">
            <a class="hover-white white-90 no-underline" href="/blog/" title="Blog page">
              Blog
            </a>
          </li>
          
        </ul>
      
      <div class="ananke-socials"></div>

    </div>
  </div>
</nav>

    </div>
  </header>



    <main class="pb7" role="main">
      
  
  
  <article class="flex-l mw8 center ph3 flex-wrap justify-between">
    <header class="mt4 w-100"><div id="sharing" class="mt3 ananke-socials"></div>
<h1 class="f1 athelas mt3 mb1">Publications</h1>
      
      
      

      
      
    </header>
    <div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><h2 id="selected-publications">Selected Publications</h2>
<p>title=&ldquo;Encoding models in functional magnetic resonance imaging: the Voxelwise Encoding Model framework (Visconti di Oleggio Castello, Deniz, et al., PsyArXiv preprint)&rdquo;
date=&ldquo;2025-09-17&rdquo;
url=&ldquo;<a href="https://www.psyarxiv.com/nt2jq%22">https://www.psyarxiv.com/nt2jq&quot;</a>
image=&quot;/img/papers/ViscontidOC.Deniz.2025.webp&rdquo;
alt=&ldquo;Visconti di Oleggio Castello et al. 2025 VEM framework&rdquo;
description=&ldquo;This paper provides the first comprehensive guide for creating encoding models with fMRI data, and complements our VEM tutorials. The Voxelwise Encoding Model (VEM) framework extracts features from stimuli or tasks and uses them in encoding models to predict brain activity. When models successfully predict activity in brain regions, we can conclude that information represented in the features is also encoded in those regions. This comprehensive guide makes this powerful methodology accessible to researchers at all levels.&rdquo;
     

<div class="publication-card card fade-in">
  <div class="publication-image">
    <a href="" target="_blank" rel="noopener noreferrer">
      <img src="" alt="" />
    </a>
  </div>
  <div class="publication-info">
    <h3 class="publication-title">
      <a href="" target="_blank" rel="noopener noreferrer"></a>
    </h3>
    <p class="publication-date"></p>
    <div class="publication-description"></div>
  </div>
</div>
</p>
<p>title=&ldquo;Bilingual language processing relies on shared semantic representations that are modulated by each language (Chen et al., bioRxiv preprint)&rdquo;
date=&ldquo;2024-11-21&rdquo;
url=&ldquo;<a href="https://www.biorxiv.org/content/biorxiv/early/2024/11/21/2024.06.24.600505.full.pdf%22">https://www.biorxiv.org/content/biorxiv/early/2024/11/21/2024.06.24.600505.full.pdf&quot;</a>
image=&quot;/img/papers/Chen.etal.2024.2.webp&rdquo;
alt=&ldquo;Chen bilingual 2024&rdquo;
description=&ldquo;Billions of people throughout the world are bilingual and can extract meaning from multiple languages. To determine how semantic representations in the brains of bilinguals can support both shared and distinct processing for different languages, we performed fMRI scans of participants who are fluent in both English and Chinese while they read natural narratives in each language. We find that semantic representations are largely shared between languages. However, there are finer-grained differences that systematically alter how the same meaning is represented between different languages. Thus, semantic brain representations in bilinguals are shared across languages but modulated by each language.&rdquo;
     

<div class="publication-card card fade-in">
  <div class="publication-image">
    <a href="" target="_blank" rel="noopener noreferrer">
      <img src="" alt="" />
    </a>
  </div>
  <div class="publication-info">
    <h3 class="publication-title">
      <a href="" target="_blank" rel="noopener noreferrer"></a>
    </h3>
    <p class="publication-date"></p>
    <div class="publication-description"></div>
  </div>
</div>
</p>
<p>title=&ldquo;Individual differences shape conceptual representation in the brain (Visconti di Oleggio Castello et al., bioRxiv preprint)&rdquo;
date=&ldquo;2025-08-22&rdquo;
url=&ldquo;<a href="https://www.biorxiv.org/content/10.1101/2025.08.22.671848v1%22">https://www.biorxiv.org/content/10.1101/2025.08.22.671848v1&quot;</a>
image=&quot;/img/papers/ViscontiDoC.2025.webp&rdquo;
alt=&ldquo;Visconti di Oleggio Castello 2025&rdquo;
description=&ldquo;Current cognitive neuroscience studies typically focus on group averages, ignoring meaningful individual differences that are crucial for developing personalized medical interventions. In this study we develop a new computational framework to measure and interpret individual differences in functional brain maps, and use it to identify individual differences in conceptual representation. We found robust individual differences that reflect cognitive traits unique to each person. This framework enables new precision neuroscience approaches to the study of complex functional representations.&rdquo;
     

<div class="publication-card card fade-in">
  <div class="publication-image">
    <a href="" target="_blank" rel="noopener noreferrer">
      <img src="" alt="" />
    </a>
  </div>
  <div class="publication-info">
    <h3 class="publication-title">
      <a href="" target="_blank" rel="noopener noreferrer"></a>
    </h3>
    <p class="publication-date"></p>
    <div class="publication-description"></div>
  </div>
</div>
</p>
<p>title=&ldquo;The Voxelwise Encoding Model framework: A tutorial introduction to fitting encoding models to fMRI data (Dupré la Tour et al., Imaging Neuroscience)&rdquo;
date=&ldquo;2025-05-09&rdquo;
url=&ldquo;<a href="https://doi.org/10.1162/imag_a_00575%22">https://doi.org/10.1162/imag_a_00575&quot;</a>
image=&quot;/img/papers/DuprelaTour.T.2025.webp&rdquo;
alt=&ldquo;Dupré la Tour 2025&rdquo;
description=&ldquo;This comprehensive tutorial provides practical guidance on using the Voxelwise Encoding Model (VEM) framework for functional brain mapping. The VEM approach extracts features from stimuli or tasks and uses them in encoding models to predict brain activity. When models successfully predict activity in brain regions, we can conclude that information represented in the features is also encoded in those regions. The tutorial includes hands-on examples with public datasets, code repositories, and interactive notebooks to make this powerful methodology accessible to researchers at all levels.&rdquo;
     

<div class="publication-card card fade-in">
  <div class="publication-image">
    <a href="" target="_blank" rel="noopener noreferrer">
      <img src="" alt="" />
    </a>
  </div>
  <div class="publication-info">
    <h3 class="publication-title">
      <a href="" target="_blank" rel="noopener noreferrer"></a>
    </h3>
    <p class="publication-date"></p>
    <div class="publication-description"></div>
  </div>
</div>
</p>
<p>title=&ldquo;The cortical representation of language timescales is shared between reading and listening (Chen et al., Communications Biology)&rdquo;
date=&ldquo;2024-07-01&rdquo;
url=&ldquo;<a href="https://www.nature.com/articles/s42003-024-05909-z.pdf%22">https://www.nature.com/articles/s42003-024-05909-z.pdf&quot;</a>
image=&quot;/img/papers/Chen.etal.2024.webp&rdquo;
alt=&ldquo;Chen 2024&rdquo;
description=&ldquo;Language comprehension involves integrating low-level sensory inputs into a hierarchy of increasingly high-level features. To recover this hierarchy we mapped the intrinsic timescale of language representation across the cerebral cortex during listening and reading. We find that the timescale of representation is organized similarly for the two modalities.&rdquo;
     

<div class="publication-card card fade-in">
  <div class="publication-image">
    <a href="" target="_blank" rel="noopener noreferrer">
      <img src="" alt="" />
    </a>
  </div>
  <div class="publication-info">
    <h3 class="publication-title">
      <a href="" target="_blank" rel="noopener noreferrer"></a>
    </h3>
    <p class="publication-date"></p>
    <div class="publication-description"></div>
  </div>
</div>
</p>
<p>title=&ldquo;Semantic representations during language comprehension are affected by context (Deniz et al., Journal of Neuroscience)&rdquo;
date=&ldquo;2023-04-26&rdquo;
url=&ldquo;<a href="https://www.jneurosci.org/content/jneuro/43/17/3144.full.pdf%22">https://www.jneurosci.org/content/jneuro/43/17/3144.full.pdf&quot;</a>
image=&quot;/img/papers/Deniz.F.2023.webp&rdquo;
alt=&ldquo;Deniz 2023&rdquo;
description=&ldquo;Context is an important part of understanding the meaning of natural language, but most neuroimaging studies of meaning use isolated words and isolated sentences with little context. In this study, we examined whether the results of neuroimaging language studies that use out-of-context stimuli generalize to natural language. We find that increasing context improves the quality of neuroimaging data and changes where and how semantic information is represented in the brain. These results suggest that findings from studies using out-of-context stimuli may not generalize to natural language used in daily life.&rdquo;
     

<div class="publication-card card fade-in">
  <div class="publication-image">
    <a href="" target="_blank" rel="noopener noreferrer">
      <img src="" alt="" />
    </a>
  </div>
  <div class="publication-info">
    <h3 class="publication-title">
      <a href="" target="_blank" rel="noopener noreferrer"></a>
    </h3>
    <p class="publication-date"></p>
    <div class="publication-description"></div>
  </div>
</div>
</p>
<p>title=&ldquo;Phonemic segmentation of narrative speech in human cerebral cortex (Gong et al., Nature Communications)&rdquo;
date=&ldquo;2023-06-29&rdquo;
url=&ldquo;<a href="https://www.nature.com/articles/s41467-023-39872-w.pdf%22">https://www.nature.com/articles/s41467-023-39872-w.pdf&quot;</a>
image=&quot;/img/papers/Gong.X.etal.2023.webp&rdquo;
alt=&ldquo;Gong 2023&rdquo;
description=&ldquo;Phonemes are a critical intermediate element of speech. This fMRI study identifies the brain representation of single phonemes, and of diphones and triphones. We find that many regions in and around the auditory cortex represent phonemes. These regions include classical areas in the dorsal superior temporal gyrus and a larger region in the lateral temporal cortex (where diphone features appear to be represented). Furthermore, we identify regions where phonemic processing and lexical retrieval are intertwined. (Note: this is work done in collaboration with the <!-- raw HTML omitted -->Theunissen lab<!-- raw HTML omitted --> here at UCB.)&rdquo;
     

<div class="publication-card card fade-in">
  <div class="publication-image">
    <a href="" target="_blank" rel="noopener noreferrer">
      <img src="" alt="" />
    </a>
  </div>
  <div class="publication-info">
    <h3 class="publication-title">
      <a href="" target="_blank" rel="noopener noreferrer"></a>
    </h3>
    <p class="publication-date"></p>
    <div class="publication-description"></div>
  </div>
</div>
</p>
<p>title=&ldquo;Feature-space selection with banded ridge regression (Dupré la Tour et al., Neuroimage)&rdquo;
date=&ldquo;2022-12-01&rdquo;
url=&ldquo;<a href="https://www.sciencedirect.com/science/article/pii/S1053811922008497%22">https://www.sciencedirect.com/science/article/pii/S1053811922008497&quot;</a>
image=&quot;/img/papers/DuprelaTour.T.2022.webp&rdquo;
alt=&ldquo;Dupre 2022&rdquo;
description=&ldquo;Encoding models identify the information represented in brain recordings, but fitting multiple models simultaneously presents several challenges. This paper describes how banded ridge regression can be used to solve these problems. Furthermore, several methods are proposed to address the computational challenge of fitting banded ridge regressions on large numbers of voxels and feature spaces. All implementations are released in an open-source Python package called Himalaya.&rdquo;
     

<div class="publication-card card fade-in">
  <div class="publication-image">
    <a href="" target="_blank" rel="noopener noreferrer">
      <img src="" alt="" />
    </a>
  </div>
  <div class="publication-info">
    <h3 class="publication-title">
      <a href="" target="_blank" rel="noopener noreferrer"></a>
    </h3>
    <p class="publication-date"></p>
    <div class="publication-description"></div>
  </div>
</div>
</p>
<p>title=&ldquo;Visual and linguistic semantic representations are aligned at the border of human visual cortex (Popham et al., Nature Neuroscience)&rdquo;
date=&ldquo;2021-10-28&rdquo;
url=&ldquo;<a href="https://drive.google.com/file/d/1_CcPfViYAUQFD2HxdneEzSrmBwjd-QkJ/view%22">https://drive.google.com/file/d/1_CcPfViYAUQFD2HxdneEzSrmBwjd-QkJ/view&quot;</a>
image=&quot;/img/papers/Popham.S.2021.webp&rdquo;
alt=&ldquo;Popham 2021&rdquo;
description=&ldquo;The human brain contains functionally and anatomically distinct networks for representing semantic information in each sensory modality, and a separate, distributed amodal conceptual network. In this study we examined the spatial organization of visual and amodal semantic functional maps. The pattern of semantic selectivity in these two distinct networks corresponds along the boundary of visual cortex: for visual categories represented posterior to the boundary, the same categories are represented linguistically on the anterior side. These results suggest that these two networks are smoothly joined to form one contiguous map.&rdquo;
     

<div class="publication-card card fade-in">
  <div class="publication-image">
    <a href="" target="_blank" rel="noopener noreferrer">
      <img src="" alt="" />
    </a>
  </div>
  <div class="publication-info">
    <h3 class="publication-title">
      <a href="" target="_blank" rel="noopener noreferrer"></a>
    </h3>
    <p class="publication-date"></p>
    <div class="publication-description"></div>
  </div>
</div>
</p>
<p>title=&ldquo;Voxel-based state space modeling recovers task-related cognitive states in naturalistic fMRI experiments (Zhang et al., Front. Neuro.)&rdquo;
date=&ldquo;2021-05-01&rdquo;
url=&ldquo;<a href="https://www.frontiersin.org/articles/10.3389/fnins.2020.565976/full%22">https://www.frontiersin.org/articles/10.3389/fnins.2020.565976/full&quot;</a>
image=&quot;/img/papers/Zhang.T.2021.webp&rdquo;
alt=&ldquo;Zhang 2021&rdquo;
description=&ldquo;Complex natural tasks recruit many different functional brain networks, and we understand little about how such tasks are represented in the brain. Here we present a voxel-based state space modeling method for recovering task-related state spaces from human fMRI data. We apply this method to data acquired in a controlled visual attention task and a video game task. We show that each task induces distinct brain states that can be embedded in a low-dimensional state space that reflects task parameters, and that attention increases state separation in the task-related subspace.&rdquo;
     

<div class="publication-card card fade-in">
  <div class="publication-image">
    <a href="" target="_blank" rel="noopener noreferrer">
      <img src="" alt="" />
    </a>
  </div>
  <div class="publication-info">
    <h3 class="publication-title">
      <a href="" target="_blank" rel="noopener noreferrer"></a>
    </h3>
    <p class="publication-date"></p>
    <div class="publication-description"></div>
  </div>
</div>
</p>
<p>title=&ldquo;Design of complex neuroscience experiments using mixed-integer linear programming (Slivkoff and Gallant, Neuron)&rdquo;
date=&ldquo;2021-05-05&rdquo;
url=&ldquo;<a href="https://www.cell.com/neuron/pdf/S0896-6273(21)00119-7.pdf%22">https://www.cell.com/neuron/pdf/S0896-6273(21)00119-7.pdf&quot;</a>
image=&quot;/img/papers/Slivkoff.S.2021.webp&rdquo;
alt=&ldquo;Slivkoff 2021&rdquo;
description=&ldquo;This tutorial and primer reviews how mixed integer linear programming can be used to optimize the design of complex experiments using many different variables. The approach is particularly useful when designing complex fMRI experiments&ndash;such as question answering studies&ndash;that aim to manipulate and probe many dimensions simultaneously.&rdquo;
     

<div class="publication-card card fade-in">
  <div class="publication-image">
    <a href="" target="_blank" rel="noopener noreferrer">
      <img src="" alt="" />
    </a>
  </div>
  <div class="publication-info">
    <h3 class="publication-title">
      <a href="" target="_blank" rel="noopener noreferrer"></a>
    </h3>
    <p class="publication-date"></p>
    <div class="publication-description"></div>
  </div>
</div>
</p>
<p>title=&ldquo;The representation of semantic information across human cerebral cortex during listening versus reading is invariant to stimulus modality (Deniz et al., J. Neurosci.)&rdquo;
date=&ldquo;2019-09-05&rdquo;
url=&ldquo;<a href="https://www.jneurosci.org/content/39/39/7722%22">https://www.jneurosci.org/content/39/39/7722&quot;</a>
image=&quot;/img/papers/Deniz.F.2019.webp&rdquo;
alt=&ldquo;Deniz 2019&rdquo;
description=&ldquo;Humans can comprehend the meaning of words from both spoken and written language. It is therefore important to understand the relationship between the brain representations of spoken or written text. Here, we show that although the representation of semantic information in the human brain is quite complex, the semantic representations evoked by listening versus reading are almost identical. These results suggest that the representation of language semantics is independent of the sensory modality through which the semantic information is received.&rdquo;
     

<div class="publication-card card fade-in">
  <div class="publication-image">
    <a href="" target="_blank" rel="noopener noreferrer">
      <img src="" alt="" />
    </a>
  </div>
  <div class="publication-info">
    <h3 class="publication-title">
      <a href="" target="_blank" rel="noopener noreferrer"></a>
    </h3>
    <p class="publication-date"></p>
    <div class="publication-description"></div>
  </div>
</div>
</p>
<p>title=&ldquo;Human scene-selective areas represent 3D configurations of surfaces (Lescroart et al., Neuron)&rdquo;
date=&ldquo;2019-01-02&rdquo;
url=&ldquo;<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4852309%22">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4852309&quot;</a>
image=&quot;/img/papers/Lescroart.M.2019.webp&rdquo;
alt=&ldquo;Lescroart 2019&rdquo;
description=&ldquo;It has been argued that scene-selective areas in the human brain represent both the 3D structure of the local visual environment and low-level 2D features that provide cues for 3D structure. To evaluate these hypotheses we developed an encoding model of 3D scene structure and tested it against a model of low-level 2D features. We fit the models to fMRI data recorded while subjects viewed visual scenes. Scene-selective areas represent the distance to and orientation of large surfaces. The most important dimensions of 3D structure are distance and openness.&rdquo;
     

<div class="publication-card card fade-in">
  <div class="publication-image">
    <a href="" target="_blank" rel="noopener noreferrer">
      <img src="" alt="" />
    </a>
  </div>
  <div class="publication-info">
    <h3 class="publication-title">
      <a href="" target="_blank" rel="noopener noreferrer"></a>
    </h3>
    <p class="publication-date"></p>
    <div class="publication-description"></div>
  </div>
</div>
</p>
<p>title=&ldquo;Natural speech reveals the semantic maps that tile human cerebral cortex (Huth et al., Nature)&rdquo;
date=&ldquo;2016-04-27&rdquo;
url=&ldquo;<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4852309/%22">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4852309/&quot;</a>
image=&quot;/img/papers/Huth.A.2016.webp&rdquo;
alt=&ldquo;Huth 2016&rdquo;
description=&ldquo;As of 2016 (when this paper appeared) little of the human lexical-semantic system had been mapped comprehensively, and the semantic selectivity of most regions was unknown. We collected fMRI while subjects listened to narrative stories, and recovered lexical-semantic maps by voxelwise modeling. We showed that the semantic system is organized into intricate patterns that seem to be consistent across individuals. We then used a generative model to create a detailed semantic atlas. Our results show that most areas within the semantic system represent information about groups of related concepts, and the atlas shows which concepts are represented in each area.&rdquo;
     

<div class="publication-card card fade-in">
  <div class="publication-image">
    <a href="" target="_blank" rel="noopener noreferrer">
      <img src="" alt="" />
    </a>
  </div>
  <div class="publication-info">
    <h3 class="publication-title">
      <a href="" target="_blank" rel="noopener noreferrer"></a>
    </h3>
    <p class="publication-date"></p>
    <div class="publication-description"></div>
  </div>
</div>
</p>
<hr>
<p>For a complete list of publications, visit our <a href="https://scholar.google.com/citations?user=nSZG-vcAAAAJ&amp;hl=en">Google Scholar page</a>.</p>
<ul class="pa0">
  
</ul>
<div class="mt6 instapaper_ignoref">
      
      
      </div>
    </div>

    <aside class="w-30-l mt6-l">




</aside>

  </article>

    </main>
    <footer class="bg-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-center">
    <div class="f6 fw4 white-70 tc pv2 ph3">
      &copy; Copyright 2025 Jack Gallant.
      And also by the Regents of the University of California, our benevolent overlords.
      Powered by <a href="https://gohugo.io/" target="_blank" class="white-70 hover-white">Hugo</a>.
      Hosted by <a href="https://pages.github.com/" target="_blank" class="white-70 hover-white">GitHub Pages</a>.
      
    </div>
  </div>
</footer>

  </body>
</html>
