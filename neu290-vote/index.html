<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>NEU 290 Paper Voting | Gallant Lab</title><meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content><meta name=generator content="Hugo 0.154.3"><meta name=robots content="index, follow"><link rel=stylesheet href=/ananke/css/main.min.efe4d852f731d5d1fbb87718387202a97aafd768cdcdaed0662bbe6982e91824.css><link rel=canonical href=https://gallantlab.org/neu290-vote/><meta property="og:url" content="https://gallantlab.org/neu290-vote/"><meta property="og:site_name" content="Gallant Lab"><meta property="og:title" content="NEU 290 Paper Voting"><meta property="og:description" content="Neuroscience Research at UC Berkeley"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta itemprop=name content="NEU 290 Paper Voting"><meta itemprop=description content="Neuroscience Research at UC Berkeley"><meta name=twitter:card content="summary"><meta name=twitter:title content="NEU 290 Paper Voting"><meta name=twitter:description content="Neuroscience Research at UC Berkeley"><link rel=stylesheet href=/css/custom.min.css><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel=stylesheet></head><body class="ma0 avenir bg-near-white production"><div class=site-header-banner><div class=site-header-banner-content><div class=banner-left-text><div>Gallant</div><div>Lab</div></div><video autoplay loop muted playsinline class=header-banner-image>
<source src=/img/header-brain-mobile.mp4 type=video/mp4 media="(max-width: 768px)"><source src=/img/header-brain.mp4 type=video/mp4></video><div class=banner-right-text><div>at UC</div><div>Berkeley</div></div></div></div><header><nav class=pv3 role=navigation><div class="flex-l center items-center justify-between"><div class="flex-l items-center w-100"><ul class="pl0 mr3"><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/ title="News page">News</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/people/ title="People page">People</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/publications/ title="Publications page">Publications</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/brain-viewers/ title="Brain Viewers page">Brain Viewers</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/learn/ title="Learn page">Learn</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/code/ title="Code page">Code</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/data/ title="Data page">Data</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/joinus/ title="Join Us page">Join Us</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/blog/ title="Blog page">Blog</a></li></ul><div class=ananke-socials></div></div></div></nav></header><main class=pb7 role=main><style>.vote-container{max-width:1e3px;margin:0 auto;padding:20px;font-family:-apple-system,BlinkMacSystemFont,segoe ui,Roboto,sans-serif}.vote-header{text-align:center;margin-bottom:30px;padding:20px;background:linear-gradient(135deg,rgba(0,40,85,.9),rgba(0,60,120,.85));border-radius:12px;color:#fff}.vote-header h1{margin:0 0 10px;font-size:1.8rem}.vote-header p{margin:0;opacity:.9}.selection-counter{position:sticky;top:10px;z-index:100;background:#fff;padding:15px 20px;border-radius:10px;box-shadow:0 4px 20px rgba(0,0,0,.15);margin-bottom:20px;display:flex;justify-content:space-between;align-items:center;flex-wrap:wrap;gap:10px}.counter-text{font-size:1.1rem;font-weight:600}.counter-text .count{color:#036;font-size:1.3rem}.counter-text.complete .count{color:#28a745}.counter-text.over .count{color:#dc3545}.submit-btn{background:#036;color:#fff;border:none;padding:12px 30px;border-radius:6px;font-size:1rem;font-weight:600;cursor:pointer;transition:all .2s}.submit-btn:hover:not(:disabled){background:#048;transform:translateY(-1px)}.submit-btn:disabled{background:#ccc;cursor:not-allowed}.category{margin-bottom:30px;background:#f8f9fa;border-radius:10px;overflow:hidden}.category-header{background:linear-gradient(135deg,#003366,#004488);color:#fff;padding:15px 20px;font-size:1.1rem;font-weight:600;cursor:pointer;display:flex;justify-content:space-between;align-items:center}.category-header:hover{background:linear-gradient(135deg,#004488,#0055aa)}.category-toggle{font-size:1.2rem;transition:transform .3s}.category.collapsed .category-toggle{transform:rotate(-90deg)}.category.collapsed .papers{display:none}.papers{padding:10px}.paper{background:#fff;border-radius:8px;padding:15px;margin-bottom:10px;border:2px solid transparent;transition:all .2s;cursor:pointer}.paper:hover{border-color:#036}.paper.selected{border-color:#28a745;background:#f0fff4}.paper-checkbox{display:flex;align-items:flex-start;gap:12px}.paper-checkbox input[type=checkbox]{width:20px;height:20px;margin-top:3px;cursor:pointer;accent-color:#28a745}.paper-content{flex:1}.paper-citation{font-size:.95rem;line-height:1.5;color:#333;margin-bottom:8px}.paper-citation .authors{font-weight:500}.paper-citation .title{font-style:italic}.paper-link{color:#06c;text-decoration:underline;text-decoration-thickness:1px;text-underline-offset:2px}.paper-link:hover{color:#049;text-decoration-thickness:2px;background-color:rgba(0,102,204,.1)}.paper-description{font-size:.85rem;color:#666;line-height:1.4;padding-left:10px;border-left:3px solid #ddd}.paper-votes{font-size:.8rem;color:#666;margin-top:8px;display:flex;align-items:center;gap:5px}.paper-votes .vote-bar{flex:1;max-width:150px;height:8px;background:#e9ecef;border-radius:4px;overflow:hidden}.paper-votes .vote-fill{height:100%;background:#036;border-radius:4px;transition:width .3s}.results-section{margin-top:40px;padding:20px;background:#f8f9fa;border-radius:10px}.results-section h2{margin-top:0;color:#036}.total-votes{font-size:1.1rem;margin-bottom:20px;color:#666}.top-papers{display:grid;gap:10px}.top-paper{display:flex;align-items:center;gap:15px;padding:12px 15px;background:#fff;border-radius:8px}.top-paper .rank{font-size:1.2rem;font-weight:700;color:#036;width:30px}.top-paper .paper-info{flex:1}.top-paper .paper-short{font-size:.9rem;font-weight:500}.top-paper .vote-count{font-size:1rem;font-weight:600;color:#28a745}.message{padding:15px 20px;border-radius:8px;margin-bottom:20px;display:none}.message.success{background:#d4edda;color:#155724;display:block}.message.error{background:#f8d7da;color:#721c24;display:block}.loading{text-align:center;padding:40px;color:#666}@media(max-width:600px){.selection-counter{flex-direction:column;text-align:center}.paper-citation{font-size:.85rem}.paper-description{font-size:.8rem}}</style><div class=vote-container><div class=vote-header><h1>NEU 290 Paper Selection</h1><p>Select your 12 favorite papers from the list below</p></div><div id=message class=message></div><div class=selection-counter><div class=counter-text id=counter>Selected: <span class=count>0</span> / 12</div><button class=submit-btn id=submitBtn disabled>Submit Votes</button></div><div id=papers-container><div class=loading>Loading papers...</div></div><div class=results-section id=results-section><h2>Current Results</h2><div class=total-votes id=total-votes>Total ballots cast: 0</div><div class=top-papers id=top-papers><div class=loading>Loading results...</div></div></div></div><script>const PAPERS_DATA=[{category:"1. Behavioral studies of attentional warping",papers:[{id:"liverence2011",authors:"Liverence, B. M., & Scholl, B. J.",year:2011,title:"Selective attention warps spatial representation",journal:"Psychological Science, 22(2), 131–136",url:"https://doi.org/10.1177/0956797610391895",description:"Provides clear behavioral evidence that sustained attention to an object distorts judged spatial positions of nearby stimuli, implying a warping of internal spatial maps."},{id:"zosky2020",authors:"Zosky, J. E., Vickery, T. J., Walter, K. A., & Dodd, M. D.",year:2020,title:"Object-based warping in three-dimensional environments",journal:"Journal of Vision, 20(6), 16",url:"https://doi.org/10.1167/jov.20.6.16",description:"Shows that remembered spatial locations in 3D scenes are biased around attended objects, indicating that object-based attention warps perceived 3D space."},{id:"bidelman2023",authors:"Bidelman, G. M., et al.",year:2023,title:"Continuous dynamics in behavior reveal interactions between perceptual warping and speech-in-noise perception",journal:"Frontiers in Neuroscience, 17, Article 1032369",url:"https://doi.org/10.3389/fnins.2023.1032369",description:"Links behavioral warping of perceptual categories to speech-in-noise performance, showing that the structure of warped perceptual spaces predicts listening behavior."},{id:"chapman2023",authors:"Chapman, A. F., Chunharas, C., & Störmer, V. S.",year:2023,title:"Feature-based attention warps the perception of visual features",journal:"Scientific Reports, 13(1), Article 6487",url:"https://doi.org/10.1038/s41598-023-33667-5",description:"Shows that directing attention to a particular feature dimension (e.g., color, orientation) systematically biases perceived feature values, consistent with a warping of subjective feature space."},{id:"brissenden2024",authors:"Brissenden, J. A., Tobyne, S. M., & Somers, D. C.",year:2024,title:"Errors of attention adaptively warp spatial cognition",journal:"bioRxiv",url:"https://doi.org/10.1101/2024.03.15.585175",description:"Reports that systematic attentional errors lead to gradual, adaptive shifts in spatial working-memory representations, suggesting that error-driven learning warps spatial cognition."},{id:"trentin2024",authors:"Trentin, C., Falanga, L., Jeske, J., Olivers, C. N., & Slagter, H. A.",year:2024,title:"Action similarity warps visual feature space in working memory",journal:"Proceedings of the National Academy of Sciences, 121(49), Article e2413433121",url:"https://doi.org/10.1073/pnas.2413433121",description:"Demonstrates that planned actions distort visual working-memory representations, making items that afford similar actions more confusable in feature space."}]},{category:"2. Physiology and MRI studies of attentional warping in sensory and motor systems",papers:[{id:"goldberg1990",authors:"Goldberg, M. E., Colby, C. L., & Duhamel, J. R.",year:1990,title:"Representation of visuomotor space in the parietal lobe of the monkey",journal:"Cold Spring Harbor Symposia on Quantitative Biology, Vol. 55, pp. 729–739",url:"https://doi.org/10.1101/SQB.1990.055.01.068",description:"Summarizes studies showing remapping of the visual field in area LIP due to a complex interaction between spatial attention and eye movements."},{id:"connor1997",authors:"Connor, C. E., Preddie, D. C., Gallant, J. L., & Van Essen, D. C.",year:1997,title:"Spatial attention effects in macaque area V4",journal:"Journal of Neuroscience, 17(9), 3201–3214",url:"https://doi.org/10.1523/JNEUROSCI.17-09-03201.1997",description:"Shows that spatial attention changes both the gain and spatial structure of V4 receptive fields, indicating attention-driven distortions of spatial coding."},{id:"mcadams1999",authors:"McAdams, C. J., & Maunsell, J. H. R.",year:1999,title:"Effects of attention on orientation-tuning functions of single neurons in macaque area V4",journal:"Journal of Neuroscience, 19(1), 431–441",url:"https://doi.org/10.1523/JNEUROSCI.19-01-00431.1999",description:"Reports that attention can sharpen orientation tuning curves, effectively increasing the representational separation of attended orientations."},{id:"womelsdorf2006",authors:"Womelsdorf, T., Anton-Erxleben, K., Pieper, F., & Treue, S.",year:2006,title:"Dynamic shifts of visual receptive fields in cortical area MT by spatial attention",journal:"Nature Neuroscience, 9(9), 1156–1160",url:"https://doi.org/10.1038/nn1763",description:"Provides single-unit evidence that spatial attention shifts and reshapes MT receptive fields around attended locations, supporting dynamic remapping of sensory space."},{id:"david2008",authors:"David, S. V., Hayden, B. Y., Mazer, J. A., & Gallant, J. L.",year:2008,title:"Attention to stimulus features shifts spectral tuning of V4 neurons during natural vision",journal:"Neuron, 59(3), 509–521",url:"https://doi.org/10.1016/j.neuron.2008.05.031",description:"Demonstrates that feature-based attention can shift spectral (color) tuning of V4 neurons during natural viewing, indicating attention-dependent retuning of feature preferences."},{id:"reddy2009",authors:"Reddy, L., Kanwisher, N., & VanRullen, R.",year:2009,title:"Attention and biased competition in multi-voxel object representations",journal:"Proceedings of the National Academy of Sciences, 106(50), 21447–21452",url:"https://doi.org/10.1073/pnas.0907339106",description:"Using MVPA, shows that responses to multiple objects shift toward the pattern evoked by the attended object alone, consistent with biased competition and warping of multi-object representations."},{id:"cukur2013",authors:"Çukur, T., Nishimoto, S., Huth, A. G., & Gallant, J. L.",year:2013,title:"Attention during natural vision warps semantic representation across the human brain",journal:"Nature Neuroscience, 16(6), 763–770",url:"https://doi.org/10.1038/nn.3389",description:"Using fMRI with natural movies, shows that attending to different semantic categories systematically shifts voxel tuning, effectively warping semantic representational space toward attended categories."},{id:"klein2014",authors:"Klein, B. P., Harvey, B. M., & Dumoulin, S. O.",year:2014,title:"Attraction of position preference by spatial attention throughout human visual cortex",journal:"Neuron, 84(1), 227–237",url:"https://doi.org/10.1016/j.neuron.2014.08.040",description:"Using population receptive field mapping, finds that spatial attention pulls receptive field centers toward attended locations across visual cortex, revealing dynamic warping of retinotopic maps."},{id:"keller2022",authors:"Keller, A. S., Payne, L., & Sekuler, R.",year:2022,title:"Attention enhances category representations across the human visual hierarchy",journal:"Journal of Neuroscience, 42(2), 246–260",url:"https://doi.org/10.1523/JNEUROSCI.1217-21.2021",description:"Shows that attention expands and sharpens category-specific representations across multiple visual areas, effectively stretching category distances in representational space."},{id:"shahdloo2022",authors:"Shahdloo, M., Çelik, E., Urgen, B. A., Gallant, J. L., & Çukur, T.",year:2022,title:"Task-dependent warping of semantic representations during search for visual action categories",journal:"Journal of Neuroscience, 42(35), 6782–6799",url:"https://doi.org/10.1523/JNEUROSCI.2517-21.2022",description:"Using fMRI and encoding models, shows that semantic representations in visual cortex change shape depending on the current action-category search task, warping semantic space toward task-relevant dimensions."},{id:"doostani2023",authors:"Doostani, K., Peelen, M. V., & Hickey, C.",year:2023,title:"Attention modulates human visual responses to objects by tuning sharpening",journal:"eLife, 12, Article e89836",url:"https://doi.org/10.7554/eLife.89836",description:"Shows that attention enhances object representations primarily via sharpening of tuning curves rather than uniform gain, implying a focused warping of representational geometry."}]},{category:"3. Physiology and MRI studies of attentional warping in cognitive and memory systems",papers:[{id:"kuhl2014",authors:"Kuhl, B. A., & Chun, M. M.",year:2014,title:"Successful remembering elicits event-specific activity patterns in lateral parietal cortex",journal:"Journal of Neuroscience, 34(23), 8051–8060",url:"https://doi.org/10.1523/JNEUROSCI.5151-13.2014",description:"Shows that successful episodic retrieval is associated with event-specific patterns in lateral parietal cortex, indicating that retrieval goals modulate the structure of mnemonic representations."},{id:"ester2016",authors:"Ester, E. F., Sprague, T. C., & Serences, J. T.",year:2016,title:"Parietal and frontal cortex encode stimulus-specific mnemonic representations during visual working memory",journal:"Neuron, 87(4), 893–905",url:"https://doi.org/10.1016/j.neuron.2016.07.015",description:"Shows that posterior parietal and frontal regions maintain feature-specific working-memory representations whose fidelity is modulated by attentional priority, implying flexible reshaping of mnemonic codes."},{id:"nastase2018",authors:"Nastase, S. A., Halchenko, Y. O., Connolly, A. C., Gobbini, M. I., & Haxby, J. V.",year:2018,title:"Neural responses to naturalistic clips of behaving animals in two different task contexts",journal:"Frontiers in Neuroscience, 12, Article 316",url:"https://doi.org/10.3389/fnins.2018.00316",description:"Examines how task context modulates neural responses to naturalistic animal videos, revealing context-dependent changes in representational structure in high-level visual cortex."},{id:"ebitz2020",authors:"Ebitz, R. B., Tu, J. C., & Hayden, B. Y.",year:2020,title:"Rules warp feature encoding in decision-making circuits",journal:"PLOS Biology, 18(11), Article e3000951",url:"https://doi.org/10.1371/journal.pbio.3000951",description:"Shows that changing task rules alters how features are encoded in macaque decision-related areas, effectively remapping neural feature spaces to align with current decision demands."}]},{category:"4. Theoretical, modeling and ANN studies of attentional warping",papers:[{id:"deco2004",authors:"Deco, G., & Rolls, E. T.",year:2004,title:"A neurodynamical cortical model of visual attention and invariant object recognition",journal:"Vision Research, 44(6), 621–642",url:"https://doi.org/10.1016/j.visres.2003.09.033",description:"Network model showing how attention modifies attractor dynamics to selectively amplify relevant object representations and suppress distractors, changing the effective representational geometry."},{id:"sprague2018",authors:"Sprague, T. C., Aulet, L. S., & Curtis, C. E.",year:2018,title:"Inverted encoding models assay population-level stimulus representations, not single-unit neural tuning",journal:"eNeuro, 5(3), Article e0052-18.2018",url:"https://doi.org/10.1523/ENEURO.0052-18.2018",description:"Clarifies that changes in channel response functions in inverted encoding models reflect adjustments in population-level representations, not necessarily single-neuron tuning changes."},{id:"dalal2025",authors:"Dalal, D., Vashishtha, G., Mishra, U., Kim, J., Kanda, M., Ha, H., ... & Jain, U.",year:2025,title:"Constructive distortion: Improving MLLMs with attention-guided image warping",journal:"arXiv",url:"https://doi.org/10.48550/arXiv.2510.09741",description:"Proposes attention-guided image warping for multimodal LLMs, showing that selectively distorting images around attended regions improves vision–language performance."}]},{category:"5. Physiology and MRI studies of mixed selectivity",papers:[{id:"miller1996",authors:"Miller, E. K., Erickson, C. A., & Desimone, R.",year:1996,title:"Neural mechanisms of visual working memory in prefrontal cortex of the macaque",journal:"Journal of Neuroscience, 16(16), 5154–5167",url:"https://doi.org/10.1523/JNEUROSCI.16-16-05154.1996",description:"Early study claiming to show that stimulus selectivity in prefrontal changes depending on task demands."},{id:"mante2013",authors:"Mante, V., Sussillo, D., Shenoy, K. V., & Newsome, W. T.",year:2013,title:"Context-dependent computation by recurrent dynamics in prefrontal cortex",journal:"Nature, 503(7474), 78–84",url:"https://doi.org/10.1038/nature12742",description:"Shows that PFC neurons mix stimulus and context information, and that recurrent dynamics implement context-dependent computations consistent with mixed-selective coding."},{id:"rigotti2013",authors:"Rigotti, M., Barak, O., Warden, M. R., Wang, X. J., Daw, N. D., Miller, E. K., & Fusi, S.",year:2013,title:"The importance of mixed selectivity in complex cognitive tasks",journal:"Nature, 497(7451), 585–590",url:"https://doi.org/10.1038/nature12160",description:"Provides single-unit evidence that prefrontal neurons exhibit mixed selectivity to combinations of task variables and that such coding is critical for solving complex tasks."},{id:"parthasarathy2017",authors:"Parthasarathy, A., Herikstad, R., Bong, J. H., Medina, F. S., Libedinsky, C., & Yen, S.-C.",year:2017,title:"Mixed selectivity morphs population codes in prefrontal cortex",journal:"Nature Neuroscience, 20(12), 1770–1779",url:"https://doi.org/10.1038/nn.4644",description:"Reports that prefrontal neurons with mixed selectivity allow population codes to morph across task contexts, supporting flexible behavior."},{id:"bernardi2020",authors:"Bernardi, S., Benna, M. K., Rigotti, M., Munuera, J., Fusi, S., & Salzman, C. D.",year:2020,title:"The geometry of abstraction in the hippocampus and prefrontal cortex",journal:"Cell, 183(4), 954–967",url:"https://doi.org/10.1016/j.cell.2020.10.018",description:"Shows that hippocampal and prefrontal populations use geometric codes that support abstraction, enabling generalization across conditions while maintaining discriminability."},{id:"johnston2020",authors:"Johnston, W. J., Palmer, S. E., & Freedman, D. J.",year:2020,title:"Nonlinear mixed selectivity supports reliable neural computation",journal:"PLOS Computational Biology, 16(2), Article e1007544",url:"https://doi.org/10.1371/journal.pcbi.1007544",description:"Combining modeling and data, shows that nonlinear mixed selectivity improves reliability and flexibility of neural computations in the face of noise."},{id:"flesch2022",authors:"Flesch, T., Juechems, K., Dumbalska, T., Saxe, A., & Summerfield, C.",year:2022,title:"Orthogonal representations for robust context-dependent task performance in brains and neural networks",journal:"Neuron, 110(7), 1258–1270",url:"https://doi.org/10.1016/j.neuron.2022.01.013",description:"Shows that both brains and networks use approximately orthogonal subspaces to encode different task contexts, limiting interference and aligning with mixed-selective coding."},{id:"tafazoli2025",authors:"Tafazoli, S., Bouchacourt, F. M., Ardalan, A., Markov, N. T., Uchimura, M., Mattar, M. G., Daw, N. D., & Buschman, T. J.",year:2025,title:"Building compositional tasks with shared neural subspaces",journal:"Nature",url:"https://doi.org/10.1038/s41586-024-08267-3",description:"Demonstrates that neural populations reuse shared subspaces to represent task components, enabling compositional construction of new tasks from existing building blocks."}]},{category:"6. Theoretical, modeling and ANN studies of mixed selectivity",papers:[{id:"sussillo2013",authors:"Sussillo, D., & Barak, O.",year:2013,title:"Opening the black box: Low-dimensional dynamics in high-dimensional recurrent neural networks",journal:"Neural Computation, 25(3), 626–649",url:"https://doi.org/10.1162/NECO_a_00409",description:"Analyzes how high-dimensional mixed-selective units give rise to low-dimensional task-relevant trajectories in RNNs, clarifying how mixed selectivity supports computation."},{id:"fusi2016",authors:"Fusi, S., Miller, E. K., & Rigotti, M.",year:2016,title:"Why neurons mix: The benefits of mixed selectivity",journal:"Current Opinion in Neurobiology, 37, 66–74",url:"https://doi.org/10.1016/j.conb.2016.01.013",description:"Review explaining how mixed selectivity enables high-dimensional representations that support flexible, linearly decodable behavior."},{id:"yang2019",authors:"Yang, G. R., Joglekar, M. R., Song, H. F., Newsome, W. T., & Wang, X.-J.",year:2019,title:"Task representations in neural networks trained to perform many cognitive tasks",journal:"Nature Neuroscience, 22(2), 297–306",url:"https://doi.org/10.1038/s41593-018-0310-2",description:"Shows that networks trained on many tasks develop structured mixed-selective units and task manifolds resembling those seen in prefrontal cortex."},{id:"ito2022a",authors:"Ito, T., Klinger, T., Schultz, D., Murray, J., Cole, M., & Rigotti, M.",year:2022,title:"Compositional generalization through abstract representations in human and artificial neural networks",journal:"Advances in Neural Information Processing Systems, 35, 32225–32239",url:"https://proceedings.neurips.cc/paper_files/paper/2022/hash/d0241a47d5c420acc1a7e15e65d5bcab-Abstract-Conference.html",description:"Links compositional generalization to the formation of abstract, factorized representations in both humans and artificial networks, grounded in high-dimensional coding."},{id:"ito2022b",authors:"Ito, T., Yang, G. R., Laurent, P., Schultz, D. H., & Cole, M. W.",year:2022,title:"Constructing neural network models from brain data reveals representational transformations linked to adaptive behavior",journal:"Nature Communications, 13(1), Article 673",url:"https://doi.org/10.1038/s41467-022-28323-0",description:"By constructing brain-constrained RNNs, shows how high-dimensional mixed-selective representations transform along trajectories that support adaptive behavior."}]},{category:"7. Physiology and MRI studies of representational drift",papers:[{id:"deitch2021",authors:"Deitch, D., Rubin, A., & Ziv, Y.",year:2021,title:"Representational drift in the mouse visual cortex",journal:"Current Biology, 31(19), 4327–4339.e6",url:"https://doi.org/10.1016/j.cub.2021.07.057",description:"Shows that visual cortical population codes drift over days even for stable stimuli, while behavior remains relatively stable."},{id:"schoonover2021",authors:"Schoonover, C. E., Ohashi, S. N., Axel, R., & Fink, A. J. P.",year:2021,title:"Representational drift in primary olfactory cortex",journal:"Nature, 594(7864), 541–546",url:"https://doi.org/10.1038/s41586-021-03561-2",description:"Provides evidence that odor representations in primary olfactory cortex drift over time, yet animals maintain reliable odor-guided behavior."},{id:"sadeh2022",authors:"Sadeh, S., & Clopath, C.",year:2022,title:"Contribution of behavioural variability to representational drift",journal:"eLife, 11, Article e77907",url:"https://doi.org/10.7554/eLife.77907",description:"Combining data and modeling, argues that behavioral variability and strategy changes explain a significant portion of observed representational drift."}]},{category:"8. Theoretical, modeling and ANN studies of representational drift",papers:[{id:"rule2021",authors:"Rule, M. E., Ocker, G. K., & O'Leary, T.",year:2021,title:"Self-healing codes: How stable neural representations can arise from unstable neurons",journal:"Current Opinion in Neurobiology, 70, 121–129",url:"https://doi.org/10.1016/j.conb.2021.09.004",description:"Proposes theoretical 'self-healing' coding schemes in which population-level representations stay stable even as individual neurons' tuning drifts."},{id:"aitken2022",authors:"Aitken, K., Garrett, M., Olsen, S., & Mihalas, S.",year:2022,title:"The geometry of representational drift in natural and artificial neural networks",journal:"PLOS Computational Biology, 18(12), Article e1010716",url:"https://doi.org/10.1371/journal.pcbi.1010716",description:"Analyzes drift in both biological and artificial networks, showing that while individual units change, the geometry of population representations can remain relatively preserved."},{id:"driscoll2022",authors:"Driscoll, L. N., Duncker, L., & Harvey, C. D.",year:2022,title:"Representational drift: Emerging theories for continual learning and experimental future directions",journal:"Current Opinion in Neurobiology, 76, Article 102609",url:"https://doi.org/10.1016/j.conb.2022.102609",description:"Review discussing how drift might support continual learning and what mechanisms could allow stable behavior despite ongoing changes in neural codes."}]},{category:"9. Physiology and MRI studies of motor learning",papers:[{id:"nudo1996",authors:"Nudo, R. J., Wise, B. M., SiFuentes, F., & Milliken, G. W.",year:1996,title:"Neural substrates for the effects of rehabilitative training on motor recovery after ischemic infarct",journal:"Science, 272(5269), 1791–1794",url:"https://doi.org/10.1126/science.272.5269.1791",description:"Researchers used animal models to show that post-stroke rehabilitative training expands cortical motor maps adjacent to infarcted areas. This demonstrates use-dependent plasticity as a key mechanism for functional recovery."},{id:"wiestler2013",authors:"Wiestler, T., & Diedrichsen, J.",year:2013,title:"Skill learning strengthens cortical representations of motor sequences",journal:"eLife, 2, Article e00801",url:"https://doi.org/10.7554/eLife.00801",description:"fMRI data reveal that finger sequence training enhances effector-independent representations in M1 and superior parietal lobule. Skill acquisition reorganizes cortical maps to support sequence-specific neural tuning."},{id:"stormer2025",authors:"Störmer, V. S., & Seger, C. A.",year:2025,title:"Emergence of categorical representations in parietal and ventromedial prefrontal cortex across extended training",journal:"Journal of Neuroscience, 45(9), Article e1315242024",url:"https://doi.org/10.1523/JNEUROSCI.1315-24.2024",description:"Using fMRI during prolonged category learning tasks, this paper tracks the development of abstract representations in parietal and prefrontal areas. Categorical coding strengthens with training, aiding generalization beyond specific stimuli."}]},{category:"10. Theoretical, modeling and ANN studies of motor learning",papers:[{id:"imamizu2009",authors:"Imamizu, H., & Kawato, M.",year:2009,title:"Brain mechanisms for predictive control by switching internal models: Implications for higher-order cognitive functions",journal:"Psychological Research, 73(4), 527–544",url:"https://doi.org/10.1007/s00426-008-0217-2",description:"Models multiple paired internal models in cerebellum for context-dependent control switching. Extends to cognitive functions like language via modular predictive architectures."},{id:"krakauer2011",authors:"Krakauer, J. W., & Mazzoni, P.",year:2011,title:"Human sensorimotor learning: Adaptation, skill, and beyond",journal:"Current Opinion in Neurobiology, 21(4), 636–644",url:"https://doi.org/10.1016/j.conb.2011.06.012",description:"Distinguishes adaptation (error-driven dynamics updates) from skill learning (effector-independent sequences). Proposes distinct neural bases and calls for integrative models."},{id:"kraeutner2024",authors:"Kraeutner, S. N., Frank, C., Rieger, M., & Boe, S. G.",year:2024,title:"Imagery and motor learning: A special issue on the neurocognitive mechanisms of imagery and imagery practice of motor actions",journal:"Psychological Research, 88(6), 1785–1789",url:"https://doi.org/10.1007/s00426-024-01982-5",description:"Editorial introduces mechanisms linking motor imagery to learning, emphasizing representational overlap. It previews empirical work on imagery's role in skill acquisition and simulation."}]}],JSONBLOB_ID="019c0c84-4f43-75d7-8f31-d33435e230f8",JSONBLOB_API="https://jsonblob.com/api/jsonBlob";let selectedPapers=new Set,votesData={votes:[],paperCounts:{}},hasVoted=localStorage.getItem("neu290_voted")==="true";document.addEventListener("DOMContentLoaded",async()=>{renderPapers(),await loadVotes(),updateUI(),hasVoted&&(showMessage("You have already submitted your votes. You can view the results below.","success"),document.getElementById("submitBtn").textContent="Already Voted")});async function loadVotes(){try{const e=await fetch(`${JSONBLOB_API}/${JSONBLOB_ID}`,{method:"GET",headers:{Accept:"application/json"}});e.ok?votesData=await e.json():console.error("Failed to load votes:",e.status)}catch(e){console.error("Failed to load votes:",e)}updateResults()}async function saveVotes(){const e=await fetch(`${JSONBLOB_API}/${JSONBLOB_ID}`,{method:"PUT",headers:{"Content-Type":"application/json",Accept:"application/json"},body:JSON.stringify(votesData)});if(!e.ok){const t=await e.text();throw console.error("Save failed:",e.status,t),new Error(`Failed to save votes: ${e.status}`)}}function renderPapers(){const e=document.getElementById("papers-container");e.innerHTML="",PAPERS_DATA.forEach((t)=>{const s=document.createElement("div");s.className="category";const o=document.createElement("div");o.className="category-header",o.innerHTML=`
      <span>${t.category}</span>
      <span class="category-toggle">▼</span>
    `,o.onclick=()=>s.classList.toggle("collapsed");const i=document.createElement("div");i.className="papers",t.papers.forEach(e=>{const t=document.createElement("div");t.className="paper",t.id=`paper-${e.id}`,t.innerHTML=`
        <div class="paper-checkbox">
          <input type="checkbox" id="check-${e.id}" ${hasVoted?"disabled":""}>
          <div class="paper-content">
            <div class="paper-citation">
              <span class="authors">${e.authors}</span> (${e.year}).
              <a href="${e.url}" target="_blank" rel="noopener" class="paper-link" onclick="event.stopPropagation();">
                <span class="title">${e.title}</span>
              </a>.
              ${e.journal}.
            </div>
            <div class="paper-description">${e.description}</div>
            <div class="paper-votes">
              <span class="vote-count-text" id="votes-${e.id}">0 votes</span>
              <div class="vote-bar">
                <div class="vote-fill" id="bar-${e.id}" style="width: 0%"></div>
              </div>
            </div>
          </div>
        </div>
      `;const n=t.querySelector('input[type="checkbox"]');t.onclick=s=>{s.target.type!=="checkbox"&&!hasVoted&&(n.checked=!n.checked,handlePaperToggle(e.id,n.checked,t))},n.onchange=s=>{s.stopPropagation(),handlePaperToggle(e.id,n.checked,t)},i.appendChild(t)}),s.appendChild(o),s.appendChild(i),e.appendChild(s)})}function handlePaperToggle(e,t,n){if(t){if(selectedPapers.size>=12){const e=n.querySelector('input[type="checkbox"]');e.checked=!1,showMessage("You can only select 12 papers. Please deselect one first.","error"),setTimeout(()=>hideMessage(),3e3);return}selectedPapers.add(e),n.classList.add("selected")}else selectedPapers.delete(e),n.classList.remove("selected");updateUI()}function updateUI(){const e=document.getElementById("counter"),t=document.getElementById("submitBtn"),n=selectedPapers.size;e.innerHTML=`Selected: <span class="count">${n}</span> / 12`,e.className="counter-text",n===12?(e.classList.add("complete"),t.disabled=hasVoted):n>12?(e.classList.add("over"),t.disabled=!0):t.disabled=!0}function updateResults(){const s=document.getElementById("total-votes"),e=document.getElementById("top-papers"),t=votesData.votes?votesData.votes.length:0;s.textContent=`Total ballots cast: ${t}`;const n=PAPERS_DATA.flatMap(e=>e.papers),o=Math.max(1,...Object.values(votesData.paperCounts||{}));n.forEach(e=>{const t=votesData.paperCounts?.[e.id]||0,n=document.getElementById(`votes-${e.id}`),s=document.getElementById(`bar-${e.id}`);n&&(n.textContent=`${t} vote${t!==1?"s":""}`),s&&(s.style.width=`${t/o*100}%`)});const i=n.map(e=>({...e,votes:votesData.paperCounts?.[e.id]||0})).sort((e,t)=>t.votes-e.votes).slice(0,12);if(t===0){e.innerHTML='<div style="color: #666; padding: 20px;">No votes yet. Be the first to vote!</div>';return}e.innerHTML=i.map((e,t)=>`
    <div class="top-paper">
      <div class="rank">#${t+1}</div>
      <div class="paper-info">
        <div class="paper-short">${e.authors.split(",")[0]} et al. (${e.year})</div>
      </div>
      <div class="vote-count">${e.votes} vote${e.votes!==1?"s":""}</div>
    </div>
  `).join("")}document.getElementById("submitBtn").onclick=async()=>{if(selectedPapers.size!==12){showMessage("Please select exactly 12 papers.","error");return}if(hasVoted){showMessage("You have already voted.","error");return}const e=document.getElementById("submitBtn");e.disabled=!0,e.textContent="Submitting...";try{await loadVotes();const t={papers:Array.from(selectedPapers),timestamp:(new Date).toISOString()};votesData.votes=votesData.votes||[],votesData.votes.push(t),votesData.paperCounts=votesData.paperCounts||{},selectedPapers.forEach(e=>{votesData.paperCounts[e]=(votesData.paperCounts[e]||0)+1}),await saveVotes(),localStorage.setItem("neu290_voted","true"),hasVoted=!0,document.querySelectorAll('input[type="checkbox"]').forEach(e=>e.disabled=!0),showMessage("Your votes have been submitted successfully!","success"),e.textContent="Already Voted",updateResults()}catch(t){console.error("Failed to submit votes:",t),showMessage("Failed to submit votes. Please try again.","error"),e.disabled=!1,e.textContent="Submit Votes"}};function showMessage(e,t){const n=document.getElementById("message");n.textContent=e,n.className=`message ${t}`}function hideMessage(){const e=document.getElementById("message");e.className="message"}</script></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-center"><div class="f6 fw4 white-70 tc pv2 ph3">&copy; Copyright 2026 Jack Gallant.
And also by the Regents of the University of California, our benevolent overlords.
Powered by <a href=https://gohugo.io/ target=_blank class="white-70 hover-white">Hugo</a>.
Hosted by <a href=https://pages.github.com/ target=_blank class="white-70 hover-white">GitHub Pages</a>.
Last updated: January 28, 2026.</div></div></footer></body></html>