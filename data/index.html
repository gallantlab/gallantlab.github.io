<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>Data | Gallant Lab</title>
<meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Open datasets from the Gallant Lab for neuroscience research"><meta name=generator content="Hugo 0.139.3"><meta name=robots content="index, follow"><link rel=stylesheet href=/ananke/css/main.min.d05fb5f317fcf33b3a52936399bdf6f47dc776516e1692e412ec7d76f4a5faa2.css><link rel=canonical href=https://gallantlab.org/data/><meta property="og:url" content="https://gallantlab.org/data/"><meta property="og:site_name" content="Gallant Lab"><meta property="og:title" content="Data"><meta property="og:description" content="Open datasets from the Gallant Lab for neuroscience research"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta itemprop=name content="Data"><meta itemprop=description content="Open datasets from the Gallant Lab for neuroscience research"><meta itemprop=wordCount content="534"><meta name=twitter:card content="summary"><meta name=twitter:title content="Data"><meta name=twitter:description content="Open datasets from the Gallant Lab for neuroscience research"><link rel=stylesheet href=/css/custom.min.css><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel=stylesheet></head><body class="ma0 avenir bg-near-white production"><div class=site-header-banner><div class=site-header-banner-content><div class=banner-left-text><div>Gallant</div><div>Lab</div></div><img src=/img/header-brain.gif alt="Brain visualization" class=header-banner-image><div class=banner-right-text><div>at UC</div><div>Berkeley</div></div></div></div><header><nav class=pv3 role=navigation><div class="flex-l center items-center justify-between"><div class="flex-l items-center w-100"><ul class="pl0 mr3"><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/ title="News page">News</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/people/ title="People page">People</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/publications/ title="Publications page">Publications</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/brain-viewers/ title="Brain Viewers page">Brain Viewers</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/learn/ title="Learn page">Learn</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/code/ title="Code page">Code</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/data/ title="Data page">Data</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/joinus/ title="Join Us page">Join Us</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/blog/ title="Blog page">Blog</a></li></ul><div class=ananke-socials></div></div></div></nav></header><main class=pb7 role=main><div class="flex-l mt2 mw8 center"><article class="center cf pv5 ph3 ph4-ns mw7"><header><h1 class=f1>Data</h1></header><div class="nested-copy-line-height lh-copy f4 nested-links mid-gray"><h2 id=open-data>Open Data</h2><div class="publication-card card fade-in"><div class=publication-image><a href=https://doi.gin.g-node.org/10.12751/g-node.vy1zjd/ target=_blank rel="noopener noreferrer"><img src=/img/datasets/Popham.etal.2021.webp alt="Gallant Lab Natural Short Clips fMRI Dataset" loading=lazy></a></div><div class=publication-info><h3 class=publication-title><a href=https://doi.gin.g-node.org/10.12751/g-node.vy1zjd/ target=_blank rel="noopener noreferrer">Natural Short Clips 3T fMRI Data</a></h3><div class=publication-description>This dataset contains BOLD fMRI responses from 5 human subjects viewing natural short video clips across 3 sessions over 3 separate days for each subject. This dataset has been used in multiple publications studying visual processing and semantic representation. If you publish work that uses these data please cite the following paper: <a href=https://doi.org/10.1038/s41593-021-00921-6>Popham, S. F., Huth, A. G., Bilenko, N. Y., Deniz, F., Gao, J. S., Nunez-Elizalde, A. O., & Gallant, J. L. (2021). Visual and linguistic semantic representations are aligned at the border of human visual cortex. Nature Neuroscience, 24(11), 1628-1636.</a></div></div></div><div class="publication-card card fade-in"><div class=publication-image><a href=http://crcns.org/data-sets/vc/vim-4/about-vim-4 target=_blank rel="noopener noreferrer"><img src=/img/datasets/Zhang.etal.2021.webp alt="Gallant Lab Naturalistic fMRI Dataset" loading=lazy></a></div><div class=publication-info><h3 class=publication-title><a href=http://crcns.org/data-sets/vc/vim-4/about-vim-4 target=_blank rel="noopener noreferrer">Naturalistic fMRI Data (vim-4)</a></h3><div class=publication-description>This dataset contains whole-brain BOLD fMRI responses from human subjects performing two tasks: a visual attention task (passive movie watching with attention directed to humans or vehicles) and a video game task (open-ended Counterstrike gameplay). This dataset enables analysis of task-related cognitive states in naturalistic experimental conditions. If you publish work that uses these data please cite the following paper: <a href=https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2020.565976/full>Zhang, T., Gao, J. S., Ã‡ukur, T., & Gallant, J. L. (2021). Voxel-based state space modeling recovers task-related cognitive states in naturalistic fmri experiments. Frontiers in neuroscience, 14, 565976.</a></div></div></div><div class="publication-card card fade-in"><div class=publication-image><a href=https://gin.g-node.org/denizenslab/narratives_reading_listening_fmri/src/master/chen2024_timescales target=_blank rel="noopener noreferrer"><img src=/img/datasets/Deniz.etal.2019.webp alt="Gallant Lab Semantic Listening vs Reading fMRI Dataset" loading=lazy></a></div><div class=publication-info><h3 class=publication-title><a href=https://gin.g-node.org/denizenslab/narratives_reading_listening_fmri/src/master/chen2024_timescales target=_blank rel="noopener noreferrer">Semantic Listening vs Reading fMRI Data</a></h3><div class=publication-description>This dataset contains BOLD fMRI responses from human subjects during listening and reading tasks, demonstrating that semantic information representation across cerebral cortex remains invariant to stimulus modality. This dataset enables investigation of cross-modal semantic processing in the brain. If you publish work that uses these data please cite the following paper: <a href=https://www.jneurosci.org/content/39/39/7722>Deniz, F., Nunez-Elizalde, A. O., Huth, A. G., & Gallant, J. L. (2019). The representation of semantic information across human cerebral cortex during listening versus reading is invariant to stimulus modality. Journal of Neuroscience, 39(39), 7722-7736.</a></div></div></div><div class="publication-card card fade-in"><div class=publication-image><a href=https://gin.g-node.org/gallantlab/story_listening target=_blank rel="noopener noreferrer"><img src=/img/papers/Huth.A.2016.webp alt="Gallant Lab Story Listening fMRI Dataset" loading=lazy></a></div><div class=publication-info><h3 class=publication-title><a href=https://gin.g-node.org/gallantlab/story_listening target=_blank rel="noopener noreferrer">Story Listening fMRI Data</a></h3><div class=publication-description>This dataset contains BOLD fMRI responses from human subjects listening to natural narrative stories. This dataset was used to map semantic representations across the human cerebral cortex and create detailed semantic atlases of language processing. If you publish work that uses these data please cite the following paper: <a href=https://www.nature.com/articles/nature17637>Huth, A. G., De Heer, W. A., Griffiths, T. L., Theunissen, F. E., & Gallant, J. L. (2016). Natural speech reveals the semantic maps that tile human cerebral cortex. Nature, 532(7600), 453-458.</a></div></div></div><div class="publication-card card fade-in"><div class=publication-image><a href=https://crcns.org/data-sets/vc/vim-2/about-vim-2 target=_blank rel="noopener noreferrer"><img src=/img/datasets/Nishimoto.etal.2011.webp alt="Gallant Lab Natural Movie fMRI Dataset" loading=lazy></a></div><div class=publication-info><h3 class=publication-title><a href=https://crcns.org/data-sets/vc/vim-2/about-vim-2 target=_blank rel="noopener noreferrer">Natural Movie 4T fMRI Data (vim-2)</a></h3><div class=publication-description>This dataset contains BOLD fMRI responses from 3 subjects viewing natural movies, collected across 3 sessions on separate days. This dataset was used in the landmark study reconstructing visual experiences from brain activity. If you publish work that uses these data please cite the following paper: <a href=https://www.sciencedirect.com/science/article/pii/S0960982211009377>Nishimoto, S., Vu, A. T., Naselaris, T., Benjamini, Y., Yu, B., & Gallant, J. L. (2011). Reconstructing visual experiences from brain activity evoked by natural movies. Current Biology, 21(19), 1641-1646.</a></div></div></div><div class="publication-card card fade-in"><div class=publication-image><a href=http://crcns.org/data-sets/vc/vim-1/about-vim-1 target=_blank rel="noopener noreferrer"><img src=/img/datasets/Kay.etal.2008.webp alt="Gallant Lab Natural Images fMRI Dataset" loading=lazy></a></div><div class=publication-info><h3 class=publication-title><a href=http://crcns.org/data-sets/vc/vim-1/about-vim-1 target=_blank rel="noopener noreferrer">Natural Images fMRI Data (vim-1)</a></h3><div class=publication-description>This dataset contains BOLD fMRI responses from 2 subjects viewing natural images across 70 experimental runs collected over 5 separate days. This dataset demonstrated the ability to identify which images a subject was viewing from brain activity using Bayesian reconstruction techniques. If you publish work that uses these data please cite the following paper: <a href=https://www.nature.com/articles/nature06713>Kay, K. N., Naselaris, T., Prenger, R. J., & Gallant, J. L. (2008). Identifying natural images from human brain activity. Nature, 452(7185), 352-355.</a></div></div></div></div></article></div></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-center"><div class="f6 fw4 white-70 tc pv2 ph3">&copy; Copyright 2025 Jack Gallant.
And also by the Regents of the University of California, our benevolent overlords.
Powered by <a href=https://gohugo.io/ target=_blank class="white-70 hover-white">Hugo</a>.
Hosted by <a href=https://pages.github.com/ target=_blank class="white-70 hover-white">GitHub Pages</a>.
Last updated: December 15, 2025.</div></div></footer></body></html>