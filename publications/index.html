<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>Publications | Gallant Lab</title>
<meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Gallant Lab Publications"><meta name=generator content="Hugo 0.139.3"><meta name=robots content="index, follow"><link rel=stylesheet href=/ananke/css/main.min.d05fb5f317fcf33b3a52936399bdf6f47dc776516e1692e412ec7d76f4a5faa2.css><link rel=canonical href=https://gallantlab.org/publications/><meta property="og:url" content="https://gallantlab.org/publications/"><meta property="og:site_name" content="Gallant Lab"><meta property="og:title" content="Publications"><meta property="og:description" content="Gallant Lab Publications"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta itemprop=name content="Publications"><meta itemprop=description content="Gallant Lab Publications"><meta itemprop=wordCount content="1413"><meta name=twitter:card content="summary"><meta name=twitter:title content="Publications"><meta name=twitter:description content="Gallant Lab Publications"><link rel=stylesheet href=/css/custom.min.css><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel=stylesheet></head><body class="ma0 avenir bg-near-white production"><header><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l center items-center justify-between"><div class="flex-l items-center w-100"><a href=/ class="f3 fw7 hover-white white no-underline" style=margin-right:2rem>Gallant Lab</a><ul class="pl0 mr3"><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/ title="About page">About</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/people/ title="People page">People</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/publications/ title="Publications page">Publications</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/brain-viewers/ title="Brain Viewers page">Brain Viewers</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/learn/ title="Learn page">Learn</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/code/ title="Code page">Code</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/data/ title="Data page">Data</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/joinus/ title="Join Us page">Join Us</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/blog/ title="Blog page">Blog</a></li></ul><div class=ananke-socials></div></div></div></nav></header><main class=pb7 role=main><div class="flex-l mt2 mw8 center"><article class="center cf pv5 ph3 ph4-ns mw7"><header><h1 class=f1>Publications</h1></header><div class="nested-copy-line-height lh-copy f4 nested-links mid-gray"><h2 id=selected-publications>Selected Publications</h2><div class="publication-card card fade-in"><div class=publication-image><a href=https://www.psyarxiv.com/nt2jq target=_blank rel="noopener noreferrer"><img src=/img/papers/ViscontidOC.Deniz.2025.webp alt="Visconti di Oleggio Castello et al. 2025 VEM framework"></a></div><div class=publication-info><h3 class=publication-title><a href=https://www.psyarxiv.com/nt2jq target=_blank rel="noopener noreferrer">Encoding models in functional magnetic resonance imaging: the Voxelwise Encoding Model framework (Visconti di Oleggio Castello, Deniz, et al., PsyArXiv preprint)</a></h3><p class=publication-date>September 17, 2025</p><div class=publication-description>This paper provides the first comprehensive guide for creating encoding models with fMRI data, and complements our VEM tutorials. The Voxelwise Encoding Model (VEM) framework extracts features from stimuli or tasks and uses them in encoding models to predict brain activity. When models successfully predict activity in brain regions, we can conclude that information represented in the features is also encoded in those regions. This comprehensive guide makes this powerful methodology accessible to researchers at all levels.</div></div></div><div class="publication-card card fade-in"><div class=publication-image><a href=https://www.biorxiv.org/content/10.1101/2025.08.22.671848v1 target=_blank rel="noopener noreferrer"><img src=/img/papers/ViscontiDoC.2025.webp alt="Visconti di Oleggio Castello 2025"></a></div><div class=publication-info><h3 class=publication-title><a href=https://www.biorxiv.org/content/10.1101/2025.08.22.671848v1 target=_blank rel="noopener noreferrer">Individual differences shape conceptual representation in the brain (Visconti di Oleggio Castello et al., bioRxiv preprint)</a></h3><p class=publication-date>August 22, 2025</p><div class=publication-description>Current cognitive neuroscience studies typically focus on group averages, ignoring meaningful individual differences that are crucial for developing personalized medical interventions. In this study we develop a new computational framework to measure and interpret individual differences in functional brain maps, and use it to identify individual differences in conceptual representation. We found robust individual differences that reflect cognitive traits unique to each person. This framework enables new precision neuroscience approaches to the study of complex functional representations.</div></div></div><div class="publication-card card fade-in"><div class=publication-image><a href=https://doi.org/10.1162/imag_a_00575 target=_blank rel="noopener noreferrer"><img src=/img/papers/DuprelaTour.T.2025.webp alt="Dupré la Tour 2025"></a></div><div class=publication-info><h3 class=publication-title><a href=https://doi.org/10.1162/imag_a_00575 target=_blank rel="noopener noreferrer">The Voxelwise Encoding Model framework: A tutorial introduction to fitting encoding models to fMRI data (Dupré la Tour et al., Imaging Neuroscience)</a></h3><p class=publication-date>May 9, 2025</p><div class=publication-description>This comprehensive tutorial provides practical guidance on using the Voxelwise Encoding Model (VEM) framework for functional brain mapping. The VEM approach extracts features from stimuli or tasks and uses them in encoding models to predict brain activity. When models successfully predict activity in brain regions, we can conclude that information represented in the features is also encoded in those regions. The tutorial includes hands-on examples with public datasets, code repositories, and interactive notebooks to make this powerful methodology accessible to researchers at all levels.</div></div></div><div class="publication-card card fade-in"><div class=publication-image><a href=https://www.biorxiv.org/content/biorxiv/early/2024/11/21/2024.06.24.600505.full.pdf target=_blank rel="noopener noreferrer"><img src=/img/papers/Chen.etal.2024.2.webp alt="Chen bilingual 2024"></a></div><div class=publication-info><h3 class=publication-title><a href=https://www.biorxiv.org/content/biorxiv/early/2024/11/21/2024.06.24.600505.full.pdf target=_blank rel="noopener noreferrer">Bilingual language processing relies on shared semantic representations that are modulated by each language (Chen et al., bioRxiv preprint)</a></h3><p class=publication-date>November 21, 2024</p><div class=publication-description>Billions of people throughout the world are bilingual and can extract meaning from multiple languages. To determine how semantic representations in the brains of bilinguals can support both shared and distinct processing for different languages, we performed fMRI scans of participants who are fluent in both English and Chinese while they read natural narratives in each language. We find that semantic representations are largely shared between languages. However, there are finer-grained differences that systematically alter how the same meaning is represented between different languages. Thus, semantic brain representations in bilinguals are shared across languages but modulated by each language.</div></div></div><div class="publication-card card fade-in"><div class=publication-image><a href=https://www.nature.com/articles/s42003-024-05909-z.pdf target=_blank rel="noopener noreferrer"><img src=/img/papers/Chen.etal.2024.webp alt="Chen 2024"></a></div><div class=publication-info><h3 class=publication-title><a href=https://www.nature.com/articles/s42003-024-05909-z.pdf target=_blank rel="noopener noreferrer">The cortical representation of language timescales is shared between reading and listening (Chen et al., Communications Biology)</a></h3><p class=publication-date>July 1, 2024</p><div class=publication-description>Language comprehension involves integrating low-level sensory inputs into a hierarchy of increasingly high-level features. To recover this hierarchy we mapped the intrinsic timescale of language representation across the cerebral cortex during listening and reading. We find that the timescale of representation is organized similarly for the two modalities.</div></div></div><div class="publication-card card fade-in"><div class=publication-image><a href=https://www.jneurosci.org/content/jneuro/43/17/3144.full.pdf target=_blank rel="noopener noreferrer"><img src=/img/papers/Deniz.F.2023.webp alt="Deniz 2023"></a></div><div class=publication-info><h3 class=publication-title><a href=https://www.jneurosci.org/content/jneuro/43/17/3144.full.pdf target=_blank rel="noopener noreferrer">Semantic representations during language comprehension are affected by context (Deniz et al., Journal of Neuroscience)</a></h3><p class=publication-date>April 26, 2023</p><div class=publication-description>Context is an important part of understanding the meaning of natural language, but most neuroimaging studies of meaning use isolated words and isolated sentences with little context. In this study, we examined whether the results of neuroimaging language studies that use out-of-context stimuli generalize to natural language. We find that increasing context improves the quality of neuroimaging data and changes where and how semantic information is represented in the brain. These results suggest that findings from studies using out-of-context stimuli may not generalize to natural language used in daily life.</div></div></div><div class="publication-card card fade-in"><div class=publication-image><a href=https://www.nature.com/articles/s41467-023-39872-w.pdf target=_blank rel="noopener noreferrer"><img src=/img/papers/Gong.X.etal.2023.webp alt="Gong 2023"></a></div><div class=publication-info><h3 class=publication-title><a href=https://www.nature.com/articles/s41467-023-39872-w.pdf target=_blank rel="noopener noreferrer">Phonemic segmentation of narrative speech in human cerebral cortex (Gong et al., Nature Communications)</a></h3><p class=publication-date>June 29, 2023</p><div class=publication-description>Phonemes are a critical intermediate element of speech. This fMRI study identifies the brain representation of single phonemes, and of diphones and triphones. We find that many regions in and around the auditory cortex represent phonemes. These regions include classical areas in the dorsal superior temporal gyrus and a larger region in the lateral temporal cortex (where diphone features appear to be represented). Furthermore, we identify regions where phonemic processing and lexical retrieval are intertwined. (Note: this is work done in collaboration with the <a href=http://theunissen.berkeley.edu/>Theunissen lab</a> here at UCB.)</div></div></div><div class="publication-card card fade-in"><div class=publication-image><a href=https://www.sciencedirect.com/science/article/pii/S1053811922008497 target=_blank rel="noopener noreferrer"><img src=/img/papers/DuprelaTour.T.2022.webp alt="Dupre 2022"></a></div><div class=publication-info><h3 class=publication-title><a href=https://www.sciencedirect.com/science/article/pii/S1053811922008497 target=_blank rel="noopener noreferrer">Feature-space selection with banded ridge regression (Dupré la Tour et al., Neuroimage)</a></h3><p class=publication-date>December 1, 2022</p><div class=publication-description>Encoding models identify the information represented in brain recordings, but fitting multiple models simultaneously presents several challenges. This paper describes how banded ridge regression can be used to solve these problems. Furthermore, several methods are proposed to address the computational challenge of fitting banded ridge regressions on large numbers of voxels and feature spaces. All implementations are released in an open-source Python package called Himalaya.</div></div></div><div class="publication-card card fade-in"><div class=publication-image><a href=https://drive.google.com/file/d/1_CcPfViYAUQFD2HxdneEzSrmBwjd-QkJ/view target=_blank rel="noopener noreferrer"><img src=/img/papers/Popham.S.2021.webp alt="Popham 2021"></a></div><div class=publication-info><h3 class=publication-title><a href=https://drive.google.com/file/d/1_CcPfViYAUQFD2HxdneEzSrmBwjd-QkJ/view target=_blank rel="noopener noreferrer">Visual and linguistic semantic representations are aligned at the border of human visual cortex (Popham et al., Nature Neuroscience)</a></h3><p class=publication-date>October 28, 2021</p><div class=publication-description>The human brain contains functionally and anatomically distinct networks for representing semantic information in each sensory modality, and a separate, distributed amodal conceptual network. In this study we examined the spatial organization of visual and amodal semantic functional maps. The pattern of semantic selectivity in these two distinct networks corresponds along the boundary of visual cortex: for visual categories represented posterior to the boundary, the same categories are represented linguistically on the anterior side. These results suggest that these two networks are smoothly joined to form one contiguous map.</div></div></div><div class="publication-card card fade-in"><div class=publication-image><a href=https://www.frontiersin.org/articles/10.3389/fnins.2020.565976/full target=_blank rel="noopener noreferrer"><img src=/img/papers/Zhang.T.2021.webp alt="Zhang 2021"></a></div><div class=publication-info><h3 class=publication-title><a href=https://www.frontiersin.org/articles/10.3389/fnins.2020.565976/full target=_blank rel="noopener noreferrer">Voxel-based state space modeling recovers task-related cognitive states in naturalistic fMRI experiments (Zhang et al., Front. Neuro.)</a></h3><p class=publication-date>May 1, 2021</p><div class=publication-description>Complex natural tasks recruit many different functional brain networks, and we understand little about how such tasks are represented in the brain. Here we present a voxel-based state space modeling method for recovering task-related state spaces from human fMRI data. We apply this method to data acquired in a controlled visual attention task and a video game task. We show that each task induces distinct brain states that can be embedded in a low-dimensional state space that reflects task parameters, and that attention increases state separation in the task-related subspace.</div></div></div><div class="publication-card card fade-in"><div class=publication-image><a href=https://www.cell.com/neuron/pdf/S0896-6273%2821%2900119-7.pdf target=_blank rel="noopener noreferrer"><img src=/img/papers/Slivkoff.S.2021.webp alt="Slivkoff 2021"></a></div><div class=publication-info><h3 class=publication-title><a href=https://www.cell.com/neuron/pdf/S0896-6273%2821%2900119-7.pdf target=_blank rel="noopener noreferrer">Design of complex neuroscience experiments using mixed-integer linear programming (Slivkoff and Gallant, Neuron)</a></h3><p class=publication-date>May 5, 2021</p><div class=publication-description>This tutorial and primer reviews how mixed integer linear programming can be used to optimize the design of complex experiments using many different variables. The approach is particularly useful when designing complex fMRI experiments--such as question answering studies--that aim to manipulate and probe many dimensions simultaneously.</div></div></div><div class="publication-card card fade-in"><div class=publication-image><a href=https://www.jneurosci.org/content/39/39/7722 target=_blank rel="noopener noreferrer"><img src=/img/papers/Deniz.F.2019.webp alt="Deniz 2019"></a></div><div class=publication-info><h3 class=publication-title><a href=https://www.jneurosci.org/content/39/39/7722 target=_blank rel="noopener noreferrer">The representation of semantic information across human cerebral cortex during listening versus reading is invariant to stimulus modality (Deniz et al., J. Neurosci.)</a></h3><p class=publication-date>September 5, 2019</p><div class=publication-description>Humans can comprehend the meaning of words from both spoken and written language. It is therefore important to understand the relationship between the brain representations of spoken or written text. Here, we show that although the representation of semantic information in the human brain is quite complex, the semantic representations evoked by listening versus reading are almost identical. These results suggest that the representation of language semantics is independent of the sensory modality through which the semantic information is received.</div></div></div><div class="publication-card card fade-in"><div class=publication-image><a href=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4852309 target=_blank rel="noopener noreferrer"><img src=/img/papers/Lescroart.M.2019.webp alt="Lescroart 2019"></a></div><div class=publication-info><h3 class=publication-title><a href=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4852309 target=_blank rel="noopener noreferrer">Human scene-selective areas represent 3D configurations of surfaces (Lescroart et al., Neuron)</a></h3><p class=publication-date>January 2, 2019</p><div class=publication-description>It has been argued that scene-selective areas in the human brain represent both the 3D structure of the local visual environment and low-level 2D features that provide cues for 3D structure. To evaluate these hypotheses we developed an encoding model of 3D scene structure and tested it against a model of low-level 2D features. We fit the models to fMRI data recorded while subjects viewed visual scenes. Scene-selective areas represent the distance to and orientation of large surfaces. The most important dimensions of 3D structure are distance and openness.</div></div></div><div class="publication-card card fade-in"><div class=publication-image><a href=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4852309/ target=_blank rel="noopener noreferrer"><img src=/img/papers/Huth.A.2016.webp alt="Huth 2016"></a></div><div class=publication-info><h3 class=publication-title><a href=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4852309/ target=_blank rel="noopener noreferrer">Natural speech reveals the semantic maps that tile human cerebral cortex (Huth et al., Nature)</a></h3><p class=publication-date>April 27, 2016</p><div class=publication-description>As of 2016 (when this paper appeared) little of the human lexical-semantic system had been mapped comprehensively, and the semantic selectivity of most regions was unknown. We collected fMRI while subjects listened to narrative stories, and recovered lexical-semantic maps by voxelwise modeling. We showed that the semantic system is organized into intricate patterns that seem to be consistent across individuals. We then used a generative model to create a detailed semantic atlas. Our results show that most areas within the semantic system represent information about groups of related concepts, and the atlas shows which concepts are represented in each area.</div></div></div><hr><p>For a complete list of publications, visit our <a href="https://scholar.google.com/citations?user=nSZG-vcAAAAJ&amp;hl=en">Google Scholar page</a>.</p></div></article></div></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-center"><div class="f6 fw4 white-70 tc pv2 ph3">&copy; Copyright 2025 Jack Gallant.
And also by the Regents of the University of California, our benevolent overlords.
Powered by <a href=https://gohugo.io/ target=_blank class="white-70 hover-white">Hugo</a>.
Hosted by <a href=https://pages.github.com/ target=_blank class="white-70 hover-white">GitHub Pages</a>.
Last updated: October 27, 2025.</div></div></footer></body></html>