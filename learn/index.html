<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>Learn | Gallant Lab</title>
<meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Voxelwise Encoding Model tutorials and resources"><meta name=generator content="Hugo 0.139.3"><meta name=robots content="index, follow"><link rel=stylesheet href=/ananke/css/main.min.d05fb5f317fcf33b3a52936399bdf6f47dc776516e1692e412ec7d76f4a5faa2.css><link rel=canonical href=https://gallantlab.org/learn/><meta property="og:url" content="https://gallantlab.org/learn/"><meta property="og:site_name" content="Gallant Lab"><meta property="og:title" content="Learn"><meta property="og:description" content="Voxelwise Encoding Model tutorials and resources"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta itemprop=name content="Learn"><meta itemprop=description content="Voxelwise Encoding Model tutorials and resources"><meta itemprop=wordCount content="254"><meta name=twitter:card content="summary"><meta name=twitter:title content="Learn"><meta name=twitter:description content="Voxelwise Encoding Model tutorials and resources"><link rel=stylesheet href=/css/custom.min.css><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel=stylesheet></head><body class="ma0 avenir bg-near-white production"><div class=site-header-banner><div class=site-header-banner-content><div class=banner-left-text><div>Gallant</div><div>Lab</div></div><video autoplay loop muted playsinline class=header-banner-image>
<source src=/img/header-brain-mobile.mp4 type=video/mp4 media="(max-width: 768px)"><source src=/img/header-brain.mp4 type=video/mp4></video><div class=banner-right-text><div>at UC</div><div>Berkeley</div></div></div></div><header><nav class=pv3 role=navigation><div class="flex-l center items-center justify-between"><div class="flex-l items-center w-100"><ul class="pl0 mr3"><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/ title="News page">News</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/people/ title="People page">People</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/publications/ title="Publications page">Publications</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/brain-viewers/ title="Brain Viewers page">Brain Viewers</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/learn/ title="Learn page">Learn</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/code/ title="Code page">Code</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/data/ title="Data page">Data</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/joinus/ title="Join Us page">Join Us</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/blog/ title="Blog page">Blog</a></li></ul><div class=ananke-socials></div></div></div></nav></header><main class=pb7 role=main><div class="flex-l mt2 mw8 center"><article class="center cf pv5 ph3 ph4-ns mw7"><header><h1 class=f1>Learn</h1></header><div class="nested-copy-line-height lh-copy f4 nested-links mid-gray"><h2 id=learn-about-voxelwise-encoding-models>Learn About Voxelwise Encoding Models</h2><div class="publication-card card fade-in"><div class=publication-image><a href=https://www.psyarxiv.com/nt2jq target=_blank rel="noopener noreferrer"><img src=/img/papers/ViscontidOC.Deniz.2025.webp alt="Voxelwise Encoding Model guide" loading=lazy></a></div><div class=publication-info><h3 class=publication-title><a href=https://www.psyarxiv.com/nt2jq target=_blank rel="noopener noreferrer">Voxelwise Encoding Model review paper</a></h3><div class=publication-description>This review paper provides the first comprehensive guide to the Voxelwise Encoding Model (VEM) framework. The VEM framework is a framework for fitting encoding models to fMRI data. This framework is currently the most sensitive and powerful approach available for modeling fMRI data. It can be used to fit dozens of distinct models simultaneously, each model having up to several thousand distinct features. The Voxelwise Encoding Model framework also conforms to all best practices in data science, which maximizes sensitivity, reliability and generalizability of the resulting models.</div></div></div><div class="publication-card card fade-in"><div class=publication-image><a href=https://gallantlab.org/voxelwise_tutorials target=_blank rel="noopener noreferrer"><img src=/img/other/learn.vm.webp alt="Voxelwise Encoding Model tutorials" loading=lazy></a></div><div class=publication-info><h3 class=publication-title><a href=https://gallantlab.org/voxelwise_tutorials target=_blank rel="noopener noreferrer">Voxelwise Encoding Model tutorials</a></h3><div class=publication-description>These Python tutorials show how to fit, evaluate, and interpret voxelwise encoding models on one of our public available datasets. We are providing these online tutorials here as a service to the fMRI community.</div></div></div><div class="publication-card card fade-in"><div class=publication-image><a href="https://www.youtube.com/watch?v=jobQmEJpbhY" target=_blank rel="noopener noreferrer"><img src=/img/other/learn.workshop.webp alt="Voxelwise Encoding Model CCN workshop" loading=lazy></a></div><div class=publication-info><h3 class=publication-title><a href="https://www.youtube.com/watch?v=jobQmEJpbhY" target=_blank rel="noopener noreferrer">Voxelwise Encoding Model workshop video</a></h3><div class=publication-description>At the 2021 CCN meeting we held a keynote and tutorials session on the Voxelwise Encoding Model framework. You can find a video recording of the workshop here.</div></div></div><div class="publication-card card fade-in"><div class=publication-image><a href=https://www.sciencedirect.com/science/article/pii/S1053811922008497 target=_blank rel="noopener noreferrer"><img src=/img/papers/DuprelaTour.T.2022.webp alt="Dupre 2022" loading=lazy></a></div><div class=publication-info><h3 class=publication-title><a href=https://www.sciencedirect.com/science/article/pii/S1053811922008497 target=_blank rel="noopener noreferrer">Theory paper focusing on feature space selection and banded ridge regression.</a></h3><p class=publication-date>December 1, 2022</p><div class=publication-description>Encoding models identify the information represented in brain recordings, but fitting multiple models simultaneously presents several challenges. This paper (Dupre la tour et al., Neuroimage, 2022) describes how banded ridge regression can be used to solve these problems. Furthermore, several methods are proposed to address the computational challenge of fitting banded ridge regressions on large numbers of voxels and feature spaces. All implementations are released in an open-source Python package called Himalaya.</div></div></div></div></article></div></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-center"><div class="f6 fw4 white-70 tc pv2 ph3">&copy; Copyright 2025 Jack Gallant.
And also by the Regents of the University of California, our benevolent overlords.
Powered by <a href=https://gohugo.io/ target=_blank class="white-70 hover-white">Hugo</a>.
Hosted by <a href=https://pages.github.com/ target=_blank class="white-70 hover-white">GitHub Pages</a>.
Last updated: December 18, 2025.</div></div></footer></body></html>