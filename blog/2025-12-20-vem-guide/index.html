<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>Learning and implementing Voxelwise Encoding Models: A guide to reviews, tutorials, and software | Gallant Lab</title>
<meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Voxelwise Encoding Models (VEMs) are one of the most sensitive methods available for modeling and decoding functional brain activity. In the VEM framework, features are extracted from the stimulus (or task) and used in an encoding model to predict brain activity. If the encoding model is able to predict brain activity in some part of the brain, then one may conclude that some information represented in the features is also represented in the brain."><meta name=generator content="Hugo 0.139.3"><meta name=robots content="index, follow"><meta name=author content="Jack Gallant"><link rel=stylesheet href=/ananke/css/main.min.d05fb5f317fcf33b3a52936399bdf6f47dc776516e1692e412ec7d76f4a5faa2.css><link rel=canonical href=https://gallantlab.org/blog/2025-12-20-vem-guide/><meta property="og:url" content="https://gallantlab.org/blog/2025-12-20-vem-guide/"><meta property="og:site_name" content="Gallant Lab"><meta property="og:title" content="Learning and implementing Voxelwise Encoding Models: A guide to reviews, tutorials, and software"><meta property="og:description" content="Voxelwise Encoding Models (VEMs) are one of the most sensitive methods available for modeling and decoding functional brain activity. In the VEM framework, features are extracted from the stimulus (or task) and used in an encoding model to predict brain activity. If the encoding model is able to predict brain activity in some part of the brain, then one may conclude that some information represented in the features is also represented in the brain."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="blog"><meta property="article:published_time" content="2025-12-20T00:00:00-08:00"><meta property="article:modified_time" content="2025-12-20T00:00:00-08:00"><meta property="article:tag" content="Voxelwise Encoding Models"><meta property="article:tag" content="FMRI"><meta property="article:tag" content="Tutorials"><meta property="article:tag" content="Software"><meta itemprop=name content="Learning and implementing Voxelwise Encoding Models: A guide to reviews, tutorials, and software"><meta itemprop=description content="Voxelwise Encoding Models (VEMs) are one of the most sensitive methods available for modeling and decoding functional brain activity. In the VEM framework, features are extracted from the stimulus (or task) and used in an encoding model to predict brain activity. If the encoding model is able to predict brain activity in some part of the brain, then one may conclude that some information represented in the features is also represented in the brain."><meta itemprop=datePublished content="2025-12-20T00:00:00-08:00"><meta itemprop=dateModified content="2025-12-20T00:00:00-08:00"><meta itemprop=wordCount content="1531"><meta itemprop=keywords content="Voxelwise Encoding Models,FMRI,Tutorials,Software"><meta name=twitter:card content="summary"><meta name=twitter:title content="Learning and implementing Voxelwise Encoding Models: A guide to reviews, tutorials, and software"><meta name=twitter:description content="Voxelwise Encoding Models (VEMs) are one of the most sensitive methods available for modeling and decoding functional brain activity. In the VEM framework, features are extracted from the stimulus (or task) and used in an encoding model to predict brain activity. If the encoding model is able to predict brain activity in some part of the brain, then one may conclude that some information represented in the features is also represented in the brain."><link rel=stylesheet href=/css/custom.min.css><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel=stylesheet></head><body class="ma0 avenir bg-near-white production"><div class=site-header-banner><div class=site-header-banner-content><div class=banner-left-text><div>Gallant</div><div>Lab</div></div><video autoplay loop muted playsinline class=header-banner-image>
<source src=/img/header-brain-mobile.mp4 type=video/mp4 media="(max-width: 768px)"><source src=/img/header-brain.mp4 type=video/mp4></video><div class=banner-right-text><div>at UC</div><div>Berkeley</div></div></div></div><header><nav class=pv3 role=navigation><div class="flex-l center items-center justify-between"><div class="flex-l items-center w-100"><ul class="pl0 mr3"><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/ title="News page">News</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/people/ title="People page">People</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/publications/ title="Publications page">Publications</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/brain-viewers/ title="Brain Viewers page">Brain Viewers</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/learn/ title="Learn page">Learn</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/code/ title="Code page">Code</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/data/ title="Data page">Data</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/joinus/ title="Join Us page">Join Us</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/blog/ title="Blog page">Blog</a></li></ul><div class=ananke-socials></div></div></div></nav></header><main class=pb7 role=main><article class="cf center lh-copy nested-links nested-copy-line-height blog-list-container"><header><h1 class=blog-post-title>Learning and implementing Voxelwise Encoding Models: A guide to reviews, tutorials, and software</h1><p class=blog-meta>December 20, 2025&nbsp;by Jack Gallant</p></header><div class=blog-content><p>Voxelwise Encoding Models (VEMs) are one of the most sensitive methods available for modeling and decoding functional brain activity. In the VEM framework, features are extracted from the stimulus (or task) and used in an encoding model to predict brain activity. If the encoding model is able to predict brain activity in some part of the brain, then one may conclude that some information represented in the features is also represented in the brain.</p><p>In VEM, a separate encoding model is fitted on each spatial sample (i.e., each voxel). VEM has many benefits compared to other methods for analyzing and modeling neuroimaging data. Most importantly, VEM can use large numbers of features simultaneously, which enables the analysis of complex naturalistic stimuli and tasks. Therefore, VEM can produce high-dimensional functional maps that reflect the selectivity of each voxel to large numbers of features. Moreover, because model performance is estimated on a separate test dataset not used during fitting, VEM minimizes overfitting and inflated Type I error confounds that plague other approaches, and the results of VEM generalize to new subjects and new stimuli.</p><p>Because the VEM framework is based on relatively advanced methods from statistics and data science, VEM has not yet been adopted broadly in the field. Several years ago we set out to change that by creating open source software that implements the framework, along with tutorials and reviews that guide researchers in its use an implementation. We&rsquo;ve now completed this effort, as summarized below. The first section of this blog summarizes the foundational papers that give an overview of VEM and its history. The second section summarizes the VEM tutorial. The third section describes the various pieces of software that implement VEM. (Note that detailed information on all of this software and accompanying tutorials and papers, as well as appropriate links, can be found on the <a href=/learn/>Learn</a> and <a href=/code/>Code</a> pages of our web site.)</p><h2 id=background-papers>Background papers</h2><p><strong><a href=https://osf.io/t975e>Review of Voxelwise Encoding Models</a></strong>: This review paper provides the first comprehensive guide to the Voxelwise Encoding Model (VEM) framework. The VEM framework is a complete pipeline for fitting encoding models to fMRI data. This framework is currently the most sensitive and powerful approach available for modeling and decoding fMRI data. It can be used to fit dozens of distinct models simultaneously, each model having up to several thousand distinct features. The Voxelwise Encoding Model framework also conforms to all best practices in data science, which maximizes sensitivity, reliability and generalizability of the resulting models. Although VEM is optimized for fMRI, its roots go back to the system identification approach in neurophysiology, and so the methods can also be applied to encoding models fit to other brain measurements. (Visconti di Oleggio Castello, M., Deniz, F., Dupré la Tour, T., & Gallant, J. L. Encoding models in functional magnetic resonance imaging: the Voxelwise Encoding Model framework. Currently in review as of Dec. 2025 but available as a preprint.)</p><p><strong><a href=https://www.researchgate.net/profile/Stephen-David-2/publication/7006311_Complete_functional_characterization_of_sensory_neurons_by_system_identification/links/56097a1e08ae576ce63e282b/Complete-functional-characterization-of-sensory-neurons-by-system-identification.pdf>Older review of system identification in neurophysiology</a></strong>: The VEM framework grew out of a prior approach used in neurophysiology called System Identification. That method aimed to fit encoding models to neurophysiology data, particularly sensory data. System identification provides a clear set of guidelines for combining experimental data with other knowledge about sensory function to obtain a description that optimally predicts the way that neurons process sensory information. This prediction paradigm provides an objective method for evaluating and comparing computational models. This paper provides a review of many of the system identification algorithms that have been used in sensory neurophysiology, and shows how they can be viewed as variants of a single statistical inference problem. The paper also reviews many of the practical issues that arise when applying these methods to neurophysiological experiments: stimulus selection, behavioral control, model visualization, and validation. Finally, the paper discusses several problems to which system identification has been applied recently, including one important long-term goal of sensory neuroscience: developing models of sensory systems that accurately predict neuronal responses under completely natural conditions. Reading this older paper will give you a stronger foundation in VEM and its proper historical context in neurophysiology. (Wu, M. C. K., David, S. V., & Gallant, J. L. (2006). Complete functional characterization of sensory neurons by system identification. Annu. Rev. Neurosci., 29(1), 477-505.)</p><p><strong><a href=https://www.sciencedirect.com/science/article/pii/S1053811922008497>Explanation of the mathematical algorithms and software for fitting encoding models in VEM</a></strong>: This paper describes the quantitative and theoretical foundation of Himalaya, the banded ridge regression software that forms the computational foundation for VEM. Himalaya is routinely used in our lab to fit voxelwise encoding models to very large fMRI data sets. (Dupré la Tour, T., Eickenberg, M., Nunez-Elizalde, A. O., & Gallant, J. L. (2022). Feature-space selection with banded ridge regression. NeuroImage, 264, 119728.)</p><p><strong><a href=https://www.sciencedirect.com/science/article/abs/pii/S1053811919302988>Theoretical basis of the banded ridge (a.k.a. Tikhonov) regression algorithm that underpins VEM</a></strong>: Predictive models for neural or fMRI data are often fit using regression methods that employ priors on the model parameters. One widely used method is ridge regression, which employs a spherical multivariate normal prior that assumes equal and independent variance for all parameters. However, a spherical prior is not always optimal. Expert knowledge or hypotheses about the structure of the model parameters can often be used to construct a better prior. In these cases, non-spherical multivariate normal priors can be employed using a generalized form of ridge regression known as Tikhonov regression. This paper explains the theoretical basis for Tikhonov regression, demonstrate a computationally efficient method for its application, and show several examples of how Tikhonov regression can improve predictive models for fMRI data. (Nunez-Elizalde, A. O., Huth, A. G., & Gallant, J. L. (2019). Voxelwise encoding models with non-spherical multivariate normal priors. Neuroimage, 197, 482-492.)</p><h2 id=tutorials>Tutorials</h2><p><strong><a href=https://direct.mit.edu/imag/article/doi/10.1162/imag_a_00575/128936>Voxelwise Encoding Model tutorials</a></strong>: To demystify the VEM framework and ease its dissemination, this paper presents a series of hands-on tutorials accessible to novice practitioners. The tutorials walk the reader through the VEM pipeline, step-by-step. Readers begin by defining features, then fit encoding models, and finally visualize the data on flattened cortical maps. The VEM tutorials are based on free open-source tools and public datasets, and reproduce the analysis presented in previously published work. If you are new to VEM and you want to get going quickly, this tutorial is the place to start. (Dupré la Tour, T., Visconti di Oleggio Castello, M., & Gallant, J. L. (2025). The Voxelwise Encoding Model framework: a tutorial introduction to fitting encoding models to fMRI data. Imaging Neuroscience, 3, imag_a_00575.)</p><h2 id=software>Software</h2><p><strong><a href=https://gallantlab.org/himalaya/>Himalaya software package</a></strong>: Himalaya is the computational core of the VEM framework. This Python package implements machine learning linear(ized) models, focusing on computational efficiency for large numbers of targets. Himalaya efficiently estimates encoding models on large numbers of targets (for example, thousands of voxels in an fMRI experiment), it runs on both CPU and GPU hardware, and it provides estimators that are compatible with scikit-learn&rsquo;s API. Himalaya is routinely used in our lab to fit voxelwise encoding models to very large fMRI data sets. (Dupré la Tour, T., Eickenberg, M., Nunez-Elizalde, A. O., & Gallant, J. L. (2022). Feature-space selection with banded ridge regression. NeuroImage, 264, 119728.)</p><p><strong><a href=https://gallantlab.org/pycortex/>Pycortex</a></strong>: When VEM is used to analyze the results of complex, naturalistic fMRI experiments, it produces very rich, high-dimensional data sets that cannot be visualized or interpreted easily using conventional methods. To address this problem we borrowed a method that was first developed by neuroanatomists Jaime Oliveira and David Van Essen: cortical flattening. The Pycortex software package is a python-based toolkit for surface visualization of fMRI data that deals correctly with the complications that inevitably arise when converting from the volumetric fMRI signals to a surface-based representation. The Pycortex software underlies the many <a href=/brainviewers/>brain viewers</a> that you can find on our web site, and it is also used in other systems (e.g., Neurovault) and by other groups. (Gao, J. S., Huth, A. G., Lescroart, M. D., & Gallant, J. L. (2015). Pycortex: an interactive surface visualizer for fMRI. Frontiers in neuroinformatics, 9, 23.)</p><p><strong><a href=https://gallantlab.org/autoflatten/>Autoflatten</a></strong>: One of the most difficult parts of cortical flattening is to move from the original volumetric anatomical brain to a flattened surface. This procedure proceeds in several steps. The brain is computationally removed from the skull. Then a few cuts are placed strategically around the edges of the brain and the whole hemisphere is flattened in a way that minimizes distortion. Until just a few weeks ago this process had to be performed largely by hand. However, our postdoc <a href=/people#matteo-visconti-di-oleggio-castello-phd>Dr. Matteo Visconti di Oleggio Castello</a> has now created a new software package that automates this procedure!</p><p>None of this work would have been possible without the selfless contributions of various graduate students and postdocs in our lab. In particular I want to highlight the contributions of the following people: <a href=/people#james-gao-phd>Dr. James Gao</a> and <a href=/people#alexander-huth-phd>Prof. Alex Huth</a> for development of the Pycortex software; <a href=/people#tom-dupr%C3%A9-la-tour-phd>Dr. Tom Dupré la Tour</a> for the development of Himalaya; <a href=/people#matteo-visconti-di-oleggio-castello-phd>Dr. Matteo Visconti di Oleggio Castello</a> for the development of autoflatten; <a href=/people#anwar-nunez-elizalde-phd>Dr. Anwar Nunez-Elizalde</a> and <a href=/people#alexander-huth-phd>Prof. Alex Huth</a> for the theoretical development of Tikhonov regression for VEM; <a href=/people#tom-dupr%C3%A9-la-tour-phd>Dr. Tom Dupré la Tour</a> and <a href=/people#matteo-visconti-di-oleggio-castello-phd>Dr. Matteo Visconti di Oleggio Castello</a> for the VEM tutorials; and <a href=/people#fatma-deniz-phd>Prof. Fatma Deniz</a> and <a href=/people#matteo-visconti-di-oleggio-castello-phd>Dr. Matteo Visconti di Oleggio Castello</a> for the VEM review.</p></div><div class=blog-tags-container><p>Tags:</p><a href=/tags/voxelwise-encoding-models class=blog-tag>voxelwise encoding models</a><a href=/tags/fmri class=blog-tag>fMRI</a><a href=/tags/tutorials class=blog-tag>tutorials</a><a href=/tags/software class=blog-tag>software</a></div><nav class=blog-single-navigation><div><a href=https://gallantlab.org/blog/2024-01-15-welcome-to-our-blog/>← Welcome to the Gallant Lab Blog</a></div><div></div></nav><p class=blog-back-link><a href=/blog/>← Back to all blog posts</a></p></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-center"><div class="f6 fw4 white-70 tc pv2 ph3">&copy; Copyright 2025 Jack Gallant.
And also by the Regents of the University of California, our benevolent overlords.
Powered by <a href=https://gohugo.io/ target=_blank class="white-70 hover-white">Hugo</a>.
Hosted by <a href=https://pages.github.com/ target=_blank class="white-70 hover-white">GitHub Pages</a>.
Last updated: December 21, 2025.</div></div></footer></body></html>